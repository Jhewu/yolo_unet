{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "554668a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from ultralytics.data.utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS, check_file_speeds\n",
    "from ultralytics.utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM\n",
    "from ultralytics.utils.patches import imread\n",
    "\n",
    "class CustomBaseDataset(Dataset):\n",
    "    \"\"\"ORIGINALLY INHERITS FROM THE Dataset CLASS (TORCH)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_path: Union[str, List[str]],\n",
    "        imgsz: int = 640,\n",
    "        cache: Union[bool, str] = False,\n",
    "        augment: bool = True,\n",
    "        hyp: Dict[str, Any] = DEFAULT_CFG,\n",
    "        prefix: str = \"\",\n",
    "        rect: bool = False,\n",
    "        batch_size: int = 16,\n",
    "        stride: int = 32,\n",
    "        pad: float = 0.5,\n",
    "        single_cls: bool = False,\n",
    "        classes: Optional[List[int]] = None,\n",
    "        fraction: float = 1.0,\n",
    "        channels: int = 3,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize BaseDataset with given configuration and options.\n",
    "\n",
    "        Args:\n",
    "            img_path (str | List[str]): Path to the folder containing images or list of image paths.\n",
    "            imgsz (int): Image size for resizing.\n",
    "            cache (bool | str): Cache images to RAM or disk during training.\n",
    "            augment (bool): If True, data augmentation is applied.\n",
    "            hyp (Dict[str, Any]): Hyperparameters to apply data augmentation.\n",
    "            prefix (str): Prefix to print in log messages.\n",
    "            rect (bool): If True, rectangular training is used.\n",
    "            batch_size (int): Size of batches.\n",
    "            stride (int): Stride used in the model.\n",
    "            pad (float): Padding value.\n",
    "            single_cls (bool): If True, single class training is used.\n",
    "            classes (List[int], optional): List of included classes.\n",
    "            fraction (float): Fraction of dataset to utilize.\n",
    "            channels (int): Number of channels in the images (1 for grayscale, 3 for RGB).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.img_path = img_path\n",
    "        self.imgsz = imgsz\n",
    "        self.augment = augment\n",
    "        self.single_cls = single_cls\n",
    "        self.prefix = prefix\n",
    "        self.fraction = fraction\n",
    "        self.channels = channels\n",
    "        \"\"\"\n",
    "        ### KEY CHANGE HERE: \n",
    "        YOLO already handles multi-channel images if configured with data.yaml\n",
    "        with the keyword \"channels: 4\" as an example. The problem is within the\n",
    "        BaseDataset Class it's hard coded: \n",
    "\n",
    "        if channels == 1, then cv2.IMREAD_GRAYSCALE otherwise everything to cv2.IMREAD_COLOR\n",
    "\n",
    "        To modify it for multi-channel images, use cv2.IMREAD_UNCHANGED leaving the\n",
    "        responsibility to the user\n",
    "        ### KEY CHANGE HERE: \n",
    "        \"\"\"\n",
    "        self.cv2_flag = cv2.IMREAD_GRAYSCALE if channels == 1 else cv2.IMREAD_UNCHANGED \n",
    "        self.im_files = self.get_img_files(self.img_path)\n",
    "        self.labels = self.get_labels()\n",
    "        self.update_labels(include_class=classes)  # single_cls and include_class\n",
    "        self.ni = len(self.labels)  # number of images\n",
    "        self.rect = rect\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        if self.rect:\n",
    "            assert self.batch_size is not None\n",
    "            self.set_rectangle()\n",
    "\n",
    "        # Buffer thread for mosaic images\n",
    "        self.buffer = []  # buffer size = batch size\n",
    "        self.max_buffer_length = min((self.ni, self.batch_size * 8, 1000)) if self.augment else 0\n",
    "\n",
    "        # Cache images (options are cache = True, False, None, \"ram\", \"disk\")\n",
    "        self.ims, self.im_hw0, self.im_hw = [None] * self.ni, [None] * self.ni, [None] * self.ni\n",
    "        self.npy_files = [Path(f).with_suffix(\".npy\") for f in self.im_files]\n",
    "        self.cache = cache.lower() if isinstance(cache, str) else \"ram\" if cache is True else None\n",
    "        if self.cache == \"ram\" and self.check_cache_ram():\n",
    "            if hyp.deterministic:\n",
    "                LOGGER.warning(\n",
    "                    \"cache='ram' may produce non-deterministic training results. \"\n",
    "                    \"Consider cache='disk' as a deterministic alternative if your disk space allows.\"\n",
    "                )\n",
    "            self.cache_images()\n",
    "        elif self.cache == \"disk\" and self.check_cache_disk():\n",
    "            self.cache_images()\n",
    "\n",
    "        # Transforms\n",
    "        self.transforms = self.build_transforms(hyp=hyp)\n",
    "\n",
    "    def get_img_files(self, img_path: Union[str, List[str]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Read image files from the specified path.\n",
    "\n",
    "        Args:\n",
    "            img_path (str | List[str]): Path or list of paths to image directories or files.\n",
    "\n",
    "        Returns:\n",
    "            (List[str]): List of image file paths.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If no images are found or the path doesn't exist.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            f = []  # image files\n",
    "            for p in img_path if isinstance(img_path, list) else [img_path]:\n",
    "                p = Path(p)  # os-agnostic\n",
    "                if p.is_dir():  # dir\n",
    "                    f += glob.glob(str(p / \"**\" / \"*.*\"), recursive=True)\n",
    "                    # F = list(p.rglob('*.*'))  # pathlib\n",
    "                elif p.is_file():  # file\n",
    "                    with open(p, encoding=\"utf-8\") as t:\n",
    "                        t = t.read().strip().splitlines()\n",
    "                        parent = str(p.parent) + os.sep\n",
    "                        f += [x.replace(\"./\", parent) if x.startswith(\"./\") else x for x in t]  # local to global path\n",
    "                        # F += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"{self.prefix}{p} does not exist\")\n",
    "            im_files = sorted(x.replace(\"/\", os.sep) for x in f if x.rpartition(\".\")[-1].lower() in IMG_FORMATS)\n",
    "            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\n",
    "            assert im_files, f\"{self.prefix}No images found in {img_path}. {FORMATS_HELP_MSG}\"\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"{self.prefix}Error loading data from {img_path}\\n{HELP_URL}\") from e\n",
    "        if self.fraction < 1:\n",
    "            im_files = im_files[: round(len(im_files) * self.fraction)]  # retain a fraction of the dataset\n",
    "        check_file_speeds(im_files, prefix=self.prefix)  # check image read speeds\n",
    "        return im_files\n",
    "\n",
    "    def update_labels(self, include_class: Optional[List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Update labels to include only specified classes.\n",
    "\n",
    "        Args:\n",
    "            include_class (List[int], optional): List of classes to include. If None, all classes are included.\n",
    "        \"\"\"\n",
    "        include_class_array = np.array(include_class).reshape(1, -1)\n",
    "        for i in range(len(self.labels)):\n",
    "            if include_class is not None:\n",
    "                cls = self.labels[i][\"cls\"]\n",
    "                bboxes = self.labels[i][\"bboxes\"]\n",
    "                segments = self.labels[i][\"segments\"]\n",
    "                keypoints = self.labels[i][\"keypoints\"]\n",
    "                j = (cls == include_class_array).any(1)\n",
    "                self.labels[i][\"cls\"] = cls[j]\n",
    "                self.labels[i][\"bboxes\"] = bboxes[j]\n",
    "                if segments:\n",
    "                    self.labels[i][\"segments\"] = [segments[si] for si, idx in enumerate(j) if idx]\n",
    "                if keypoints is not None:\n",
    "                    self.labels[i][\"keypoints\"] = keypoints[j]\n",
    "            if self.single_cls:\n",
    "                self.labels[i][\"cls\"][:, 0] = 0\n",
    "\n",
    "    def load_image(self, i: int, rect_mode: bool = True) -> Tuple[np.ndarray, Tuple[int, int], Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Load an image from dataset index 'i'.\n",
    "\n",
    "        Args:\n",
    "            i (int): Index of the image to load.\n",
    "            rect_mode (bool): Whether to use rectangular resizing.\n",
    "\n",
    "        Returns:\n",
    "            im (np.ndarray): Loaded image as a NumPy array.\n",
    "            hw_original (Tuple[int, int]): Original image dimensions in (height, width) format.\n",
    "            hw_resized (Tuple[int, int]): Resized image dimensions in (height, width) format.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the image file is not found.\n",
    "        \"\"\"\n",
    "        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\n",
    "        if im is None:  # not cached in RAM\n",
    "            if fn.exists():  # load npy\n",
    "                try:\n",
    "                    im = np.load(fn)\n",
    "                except Exception as e:\n",
    "                    LOGGER.warning(f\"{self.prefix}Removing corrupt *.npy image file {fn} due to: {e}\")\n",
    "                    Path(fn).unlink(missing_ok=True)\n",
    "                    im = imread(f, flags=self.cv2_flag)  # BGR\n",
    "            else:  # read image\n",
    "                im = imread(f, flags=self.cv2_flag)  # BGR\n",
    "            if im is None:\n",
    "                raise FileNotFoundError(f\"Image Not Found {f}\")\n",
    "\n",
    "            h0, w0 = im.shape[:2]  # orig hw\n",
    "            if rect_mode:  # resize long side to imgsz while maintaining aspect ratio\n",
    "                r = self.imgsz / max(h0, w0)  # ratio\n",
    "                if r != 1:  # if sizes are not equal\n",
    "                    w, h = (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz))\n",
    "                    im = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "            elif not (h0 == w0 == self.imgsz):  # resize by stretching image to square imgsz\n",
    "                im = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)\n",
    "            if im.ndim == 2:\n",
    "                im = im[..., None]\n",
    "\n",
    "            # Add to buffer if training with augmentations\n",
    "            if self.augment:\n",
    "                self.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\n",
    "                self.buffer.append(i)\n",
    "                if 1 < len(self.buffer) >= self.max_buffer_length:  # prevent empty buffer\n",
    "                    j = self.buffer.pop(0)\n",
    "                    if self.cache != \"ram\":\n",
    "                        self.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None\n",
    "\n",
    "            return im, (h0, w0), im.shape[:2]\n",
    "\n",
    "        return self.ims[i], self.im_hw0[i], self.im_hw[i]\n",
    "\n",
    "    def cache_images(self) -> None:\n",
    "        \"\"\"Cache images to memory or disk for faster training.\"\"\"\n",
    "        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes\n",
    "        fcn, storage = (self.cache_images_to_disk, \"Disk\") if self.cache == \"disk\" else (self.load_image, \"RAM\")\n",
    "        with ThreadPool(NUM_THREADS) as pool:\n",
    "            results = pool.imap(fcn, range(self.ni))\n",
    "            pbar = TQDM(enumerate(results), total=self.ni, disable=LOCAL_RANK > 0)\n",
    "            for i, x in pbar:\n",
    "                if self.cache == \"disk\":\n",
    "                    b += self.npy_files[i].stat().st_size\n",
    "                else:  # 'ram'\n",
    "                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)\n",
    "                    b += self.ims[i].nbytes\n",
    "                pbar.desc = f\"{self.prefix}Caching images ({b / gb:.1f}GB {storage})\"\n",
    "            pbar.close()\n",
    "\n",
    "    def cache_images_to_disk(self, i: int) -> None:\n",
    "        \"\"\"Save an image as an *.npy file for faster loading.\"\"\"\n",
    "        f = self.npy_files[i]\n",
    "        if not f.exists():\n",
    "            np.save(f.as_posix(), imread(self.im_files[i]), allow_pickle=False)\n",
    "\n",
    "    def check_cache_disk(self, safety_margin: float = 0.5) -> bool:\n",
    "        \"\"\"\n",
    "        Check if there's enough disk space for caching images.\n",
    "\n",
    "        Args:\n",
    "            safety_margin (float): Safety margin factor for disk space calculation.\n",
    "\n",
    "        Returns:\n",
    "            (bool): True if there's enough disk space, False otherwise.\n",
    "        \"\"\"\n",
    "        import shutil\n",
    "\n",
    "        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes\n",
    "        n = min(self.ni, 30)  # extrapolate from 30 random images\n",
    "        for _ in range(n):\n",
    "            im_file = random.choice(self.im_files)\n",
    "            im = imread(im_file)\n",
    "            if im is None:\n",
    "                continue\n",
    "            b += im.nbytes\n",
    "            if not os.access(Path(im_file).parent, os.W_OK):\n",
    "                self.cache = None\n",
    "                LOGGER.warning(f\"{self.prefix}Skipping caching images to disk, directory not writeable\")\n",
    "                return False\n",
    "        disk_required = b * self.ni / n * (1 + safety_margin)  # bytes required to cache dataset to disk\n",
    "        total, used, free = shutil.disk_usage(Path(self.im_files[0]).parent)\n",
    "        if disk_required > free:\n",
    "            self.cache = None\n",
    "            LOGGER.warning(\n",
    "                f\"{self.prefix}{disk_required / gb:.1f}GB disk space required, \"\n",
    "                f\"with {int(safety_margin * 100)}% safety margin but only \"\n",
    "                f\"{free / gb:.1f}/{total / gb:.1f}GB free, not caching images to disk\"\n",
    "            )\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def check_cache_ram(self, safety_margin: float = 0.5) -> bool:\n",
    "        \"\"\"\n",
    "        Check if there's enough RAM for caching images.\n",
    "\n",
    "        Args:\n",
    "            safety_margin (float): Safety margin factor for RAM calculation.\n",
    "\n",
    "        Returns:\n",
    "            (bool): True if there's enough RAM, False otherwise.\n",
    "        \"\"\"\n",
    "        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes\n",
    "        n = min(self.ni, 30)  # extrapolate from 30 random images\n",
    "        for _ in range(n):\n",
    "            im = imread(random.choice(self.im_files))  # sample image\n",
    "            if im is None:\n",
    "                continue\n",
    "            ratio = self.imgsz / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio\n",
    "            b += im.nbytes * ratio**2\n",
    "        mem_required = b * self.ni / n * (1 + safety_margin)  # GB required to cache dataset into RAM\n",
    "        mem = __import__(\"psutil\").virtual_memory()\n",
    "        if mem_required > mem.available:\n",
    "            self.cache = None\n",
    "            LOGGER.warning(\n",
    "                f\"{self.prefix}{mem_required / gb:.1f}GB RAM required to cache images \"\n",
    "                f\"with {int(safety_margin * 100)}% safety margin but only \"\n",
    "                f\"{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, not caching images\"\n",
    "            )\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def set_rectangle(self) -> None:\n",
    "        \"\"\"Set the shape of bounding boxes for YOLO detections as rectangles.\"\"\"\n",
    "        bi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # batch index\n",
    "        nb = bi[-1] + 1  # number of batches\n",
    "\n",
    "        s = np.array([x.pop(\"shape\") for x in self.labels])  # hw\n",
    "        ar = s[:, 0] / s[:, 1]  # aspect ratio\n",
    "        irect = ar.argsort()\n",
    "        self.im_files = [self.im_files[i] for i in irect]\n",
    "        self.labels = [self.labels[i] for i in irect]\n",
    "        ar = ar[irect]\n",
    "\n",
    "        # Set training image shapes\n",
    "        shapes = [[1, 1]] * nb\n",
    "        for i in range(nb):\n",
    "            ari = ar[bi == i]\n",
    "            mini, maxi = ari.min(), ari.max()\n",
    "            if maxi < 1:\n",
    "                shapes[i] = [maxi, 1]\n",
    "            elif mini > 1:\n",
    "                shapes[i] = [1, 1 / mini]\n",
    "\n",
    "        self.batch_shapes = np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int) * self.stride\n",
    "        self.batch = bi  # batch index of image\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        \"\"\"Return transformed label information for given index.\"\"\"\n",
    "        return self.transforms(self.get_image_and_label(index))\n",
    "\n",
    "    def get_image_and_label(self, index: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get and return label information from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            (Dict[str, Any]): Label dictionary with image and metadata.\n",
    "        \"\"\"\n",
    "        label = deepcopy(self.labels[index])  # requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\n",
    "        label.pop(\"shape\", None)  # shape is for rect, remove it\n",
    "        label[\"img\"], label[\"ori_shape\"], label[\"resized_shape\"] = self.load_image(index)\n",
    "        label[\"ratio_pad\"] = (\n",
    "            label[\"resized_shape\"][0] / label[\"ori_shape\"][0],\n",
    "            label[\"resized_shape\"][1] / label[\"ori_shape\"][1],\n",
    "        )  # for evaluation\n",
    "        if self.rect:\n",
    "            label[\"rect_shape\"] = self.batch_shapes[self.batch[index]]\n",
    "        return self.update_labels_info(label)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the length of the labels list for the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def update_labels_info(self, label: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Custom your label format here.\"\"\"\n",
    "        return label\n",
    "\n",
    "    def build_transforms(self, hyp: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Users can customize augmentations here.\n",
    "\n",
    "        Examples:\n",
    "            >>> if self.augment:\n",
    "            ...     # Training transforms\n",
    "            ...     return Compose([])\n",
    "            >>> else:\n",
    "            ...    # Val transforms\n",
    "            ...    return Compose([])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_labels(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Users can customize their own format here.\n",
    "\n",
    "        Examples:\n",
    "            Ensure output is a dictionary with the following keys:\n",
    "            >>> dict(\n",
    "            ...     im_file=im_file,\n",
    "            ...     shape=shape,  # format: (height, width)\n",
    "            ...     cls=cls,\n",
    "            ...     bboxes=bboxes,  # xywh\n",
    "            ...     segments=segments,  # xy\n",
    "            ...     keypoints=keypoints,  # xy\n",
    "            ...     normalized=True,  # or False\n",
    "            ...     bbox_format=\"xyxy\",  # or xywh, ltwh\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba39cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils import LOGGER, NUM_THREADS, LOCAL_RANK\n",
    "from ultralytics.data.utils import verify_image_label\n",
    "from ultralytics.utils.ops import resample_segments\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "from ultralytics.utils.instance import Instances\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from ultralytics.utils import TQDM \n",
    "from typing import Optional, Dict\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from ultralytics.data.utils import (\n",
    "    HELP_URL, \n",
    "    get_hash, \n",
    "    save_dataset_cache_file, \n",
    "    img2label_paths, \n",
    "    load_dataset_cache_file\n",
    ")\n",
    "\n",
    "from ultralytics.data.augment import (\n",
    "    Compose, \n",
    "    Format,\n",
    "    LetterBox,\n",
    "    v8_transforms)\n",
    "\n",
    "DATASET_CACHE_VERSION = \"1.0.3\"\n",
    "\n",
    "class CustomYOLODataset(CustomBaseDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading object detection and/or segmentation labels in YOLO format.\n",
    "\n",
    "    This class supports loading data for object detection, segmentation, pose estimation, and oriented bounding box\n",
    "    (OBB) tasks using the YOLO format.\n",
    "\n",
    "    Attributes:\n",
    "        use_segments (bool): Indicates if segmentation masks should be used.\n",
    "        use_keypoints (bool): Indicates if keypoints should be used for pose estimation.\n",
    "        use_obb (bool): Indicates if oriented bounding boxes should be used.\n",
    "        data (dict): Dataset configuration dictionary.\n",
    "\n",
    "    Methods:\n",
    "        cache_labels: Cache dataset labels, check images and read shapes.\n",
    "        get_labels: Return dictionary of labels for YOLO training.\n",
    "        build_transforms: Build and append transforms to the list.\n",
    "        close_mosaic: Set mosaic, copy_paste and mixup options to 0.0 and build transformations.\n",
    "        update_labels_info: Update label format for different tasks.\n",
    "        collate_fn: Collate data samples into batches.\n",
    "\n",
    "    Examples:\n",
    "        >>> dataset = YOLODataset(img_path=\"path/to/images\", data={\"names\": {0: \"person\"}}, task=\"detect\")\n",
    "        >>> dataset.get_labels()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, data: Optional[Dict] = None, task: str = \"detect\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the YOLODataset.\n",
    "\n",
    "        Args:\n",
    "            data (dict, optional): Dataset configuration dictionary.\n",
    "            task (str): Task type, one of 'detect', 'segment', 'pose', or 'obb'.\n",
    "            *args (Any): Additional positional arguments for the parent class.\n",
    "            **kwargs (Any): Additional keyword arguments for the parent class.\n",
    "        \"\"\"\n",
    "        self.use_segments = task == \"segment\"\n",
    "        self.use_keypoints = task == \"pose\"\n",
    "        self.use_obb = task == \"obb\"\n",
    "        self.data = data\n",
    "\n",
    "        assert not (self.use_segments and self.use_keypoints), \"Can not use both segments and keypoints.\"\n",
    "        super().__init__(*args, channels=self.data[\"channels\"], **kwargs)\n",
    "\n",
    "    def cache_labels(self, path: Path = Path(\"./labels.cache\")) -> Dict:\n",
    "        \"\"\"\n",
    "        Cache dataset labels, check images and read shapes.\n",
    "\n",
    "        Args:\n",
    "            path (Path): Path where to save the cache file.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Dictionary containing cached labels and related information.\n",
    "        \"\"\"\n",
    "        x = {\"labels\": []}\n",
    "        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\n",
    "        desc = f\"{self.prefix}Scanning {path.parent / path.stem}...\"\n",
    "        total = len(self.im_files)\n",
    "        nkpt, ndim = self.data.get(\"kpt_shape\", (0, 0))\n",
    "        if self.use_keypoints and (nkpt <= 0 or ndim not in {2, 3}):\n",
    "            raise ValueError(\n",
    "                \"'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of \"\n",
    "                \"keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'\"\n",
    "            )\n",
    "        with ThreadPool(NUM_THREADS) as pool:\n",
    "            results = pool.imap(\n",
    "                func=verify_image_label,\n",
    "                iterable=zip(\n",
    "                    self.im_files,\n",
    "                    self.label_files,\n",
    "                    repeat(self.prefix),\n",
    "                    repeat(self.use_keypoints),\n",
    "                    repeat(len(self.data[\"names\"])),\n",
    "                    repeat(nkpt),\n",
    "                    repeat(ndim),\n",
    "                    repeat(self.single_cls),\n",
    "                ),\n",
    "            )\n",
    "            pbar = TQDM(results, desc=desc, total=total)\n",
    "            for im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n",
    "                nm += nm_f\n",
    "                nf += nf_f\n",
    "                ne += ne_f\n",
    "                nc += nc_f\n",
    "                if im_file:\n",
    "                    x[\"labels\"].append(\n",
    "                        {\n",
    "                            \"im_file\": im_file,\n",
    "                            \"shape\": shape,\n",
    "                            \"cls\": lb[:, 0:1],  # n, 1\n",
    "                            \"bboxes\": lb[:, 1:],  # n, 4\n",
    "                            \"segments\": segments,\n",
    "                            \"keypoints\": keypoint,\n",
    "                            \"normalized\": True,\n",
    "                            \"bbox_format\": \"xywh\",\n",
    "                        }\n",
    "                    )\n",
    "                if msg:\n",
    "                    msgs.append(msg)\n",
    "                pbar.desc = f\"{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt\"\n",
    "            pbar.close()\n",
    "\n",
    "        if msgs:\n",
    "            LOGGER.info(\"\\n\".join(msgs))\n",
    "        if nf == 0:\n",
    "            LOGGER.warning(f\"{self.prefix}No labels found in {path}. {HELP_URL}\")\n",
    "        x[\"hash\"] = get_hash(self.label_files + self.im_files)\n",
    "        x[\"results\"] = nf, nm, ne, nc, len(self.im_files)\n",
    "        x[\"msgs\"] = msgs  # warnings\n",
    "        save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)\n",
    "        return x\n",
    "\n",
    "    def get_labels(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Return dictionary of labels for YOLO training.\n",
    "\n",
    "        This method loads labels from disk or cache, verifies their integrity, and prepares them for training.\n",
    "\n",
    "        Returns:\n",
    "            (List[dict]): List of label dictionaries, each containing information about an image and its annotations.\n",
    "        \"\"\"\n",
    "        self.label_files = img2label_paths(self.im_files)\n",
    "        cache_path = Path(self.label_files[0]).parent.with_suffix(\".cache\")\n",
    "        try:\n",
    "            cache, exists = load_dataset_cache_file(cache_path), True  # attempt to load a *.cache file\n",
    "            assert cache[\"version\"] == DATASET_CACHE_VERSION  # matches current version\n",
    "            assert cache[\"hash\"] == get_hash(self.label_files + self.im_files)  # identical hash\n",
    "        except (FileNotFoundError, AssertionError, AttributeError):\n",
    "            cache, exists = self.cache_labels(cache_path), False  # run cache ops\n",
    "\n",
    "        # Display cache\n",
    "        nf, nm, ne, nc, n = cache.pop(\"results\")  # found, missing, empty, corrupt, total\n",
    "        if exists and LOCAL_RANK in {-1, 0}:\n",
    "            d = f\"Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt\"\n",
    "            TQDM(None, desc=self.prefix + d, total=n, initial=n)  # display results\n",
    "            if cache[\"msgs\"]:\n",
    "                LOGGER.info(\"\\n\".join(cache[\"msgs\"]))  # display warnings\n",
    "\n",
    "        # Read cache\n",
    "        [cache.pop(k) for k in (\"hash\", \"version\", \"msgs\")]  # remove items\n",
    "        labels = cache[\"labels\"]\n",
    "        if not labels:\n",
    "            raise RuntimeError(\n",
    "                f\"No valid images found in {cache_path}. Images with incorrectly formatted labels are ignored. {HELP_URL}\"\n",
    "            )\n",
    "        self.im_files = [lb[\"im_file\"] for lb in labels]  # update im_files\n",
    "\n",
    "        # Check if the dataset is all boxes or all segments\n",
    "        lengths = ((len(lb[\"cls\"]), len(lb[\"bboxes\"]), len(lb[\"segments\"])) for lb in labels)\n",
    "        len_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))\n",
    "        if len_segments and len_boxes != len_segments:\n",
    "            LOGGER.warning(\n",
    "                f\"Box and segment counts should be equal, but got len(segments) = {len_segments}, \"\n",
    "                f\"len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. \"\n",
    "                \"To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\"\n",
    "            )\n",
    "            for lb in labels:\n",
    "                lb[\"segments\"] = []\n",
    "        if len_cls == 0:\n",
    "            LOGGER.warning(f\"Labels are missing or empty in {cache_path}, training may not work correctly. {HELP_URL}\")\n",
    "        return labels\n",
    "\n",
    "    def build_transforms(self, hyp: Optional[Dict] = None) -> Compose:\n",
    "        \"\"\"\n",
    "        Build and append transforms to the list.\n",
    "\n",
    "        Args:\n",
    "            hyp (dict, optional): Hyperparameters for transforms.\n",
    "\n",
    "        Returns:\n",
    "            (Compose): Composed transforms.\n",
    "        \"\"\"\n",
    "        if self.augment:\n",
    "            hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\n",
    "            hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\n",
    "            hyp.cutmix = hyp.cutmix if self.augment and not self.rect else 0.0\n",
    "            transforms = v8_transforms(self, self.imgsz, hyp)\n",
    "        else:\n",
    "            transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])\n",
    "        transforms.append(\n",
    "            Format(\n",
    "                bbox_format=\"xywh\",\n",
    "                normalize=True,\n",
    "                return_mask=self.use_segments,\n",
    "                return_keypoint=self.use_keypoints,\n",
    "                return_obb=self.use_obb,\n",
    "                batch_idx=True,\n",
    "                mask_ratio=hyp.mask_ratio,\n",
    "                mask_overlap=hyp.overlap_mask,\n",
    "                bgr=hyp.bgr if self.augment else 0.0,  # only affect training.\n",
    "            )\n",
    "        )\n",
    "        return transforms\n",
    "\n",
    "    def close_mosaic(self, hyp: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Disable mosaic, copy_paste, mixup and cutmix augmentations by setting their probabilities to 0.0.\n",
    "\n",
    "        Args:\n",
    "            hyp (dict): Hyperparameters for transforms.\n",
    "        \"\"\"\n",
    "        hyp.mosaic = 0.0\n",
    "        hyp.copy_paste = 0.0\n",
    "        hyp.mixup = 0.0\n",
    "        hyp.cutmix = 0.0\n",
    "        self.transforms = self.build_transforms(hyp)\n",
    "\n",
    "    def update_labels_info(self, label: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Update label format for different tasks.\n",
    "\n",
    "        Args:\n",
    "            label (dict): Label dictionary containing bboxes, segments, keypoints, etc.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Updated label dictionary with instances.\n",
    "\n",
    "        Note:\n",
    "            cls is not with bboxes now, classification and semantic segmentation need an independent cls label\n",
    "            Can also support classification and semantic segmentation by adding or removing dict keys there.\n",
    "        \"\"\"\n",
    "        bboxes = label.pop(\"bboxes\")\n",
    "        segments = label.pop(\"segments\", [])\n",
    "        keypoints = label.pop(\"keypoints\", None)\n",
    "        bbox_format = label.pop(\"bbox_format\")\n",
    "        normalized = label.pop(\"normalized\")\n",
    "\n",
    "        # NOTE: do NOT resample oriented boxes\n",
    "        segment_resamples = 100 if self.use_obb else 1000\n",
    "        if len(segments) > 0:\n",
    "            # make sure segments interpolate correctly if original length is greater than segment_resamples\n",
    "            max_len = max(len(s) for s in segments)\n",
    "            segment_resamples = (max_len + 1) if segment_resamples < max_len else segment_resamples\n",
    "            # list[np.array(segment_resamples, 2)] * num_samples\n",
    "            segments = np.stack(resample_segments(segments, n=segment_resamples), axis=0)\n",
    "        else:\n",
    "            segments = np.zeros((0, segment_resamples, 2), dtype=np.float32)\n",
    "        label[\"instances\"] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format, normalized=normalized)\n",
    "        return label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Collate data samples into batches.\n",
    "\n",
    "        Args:\n",
    "            batch (List[dict]): List of dictionaries containing sample data.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Collated batch with stacked tensors.\n",
    "        \"\"\"\n",
    "        new_batch = {}\n",
    "        batch = [dict(sorted(b.items())) for b in batch]  # make sure the keys are in the same order\n",
    "        keys = batch[0].keys()\n",
    "        values = list(zip(*[list(b.values()) for b in batch]))\n",
    "        for i, k in enumerate(keys):\n",
    "            value = values[i]\n",
    "            if k in {\"img\", \"text_feats\"}:\n",
    "                value = torch.stack(value, 0)\n",
    "            elif k == \"visuals\":\n",
    "                value = torch.nn.utils.rnn.pad_sequence(value, batch_first=True)\n",
    "            if k in {\"masks\", \"keypoints\", \"bboxes\", \"cls\", \"segments\", \"obb\"}:\n",
    "                value = torch.cat(value, 0)\n",
    "            new_batch[k] = value\n",
    "        new_batch[\"batch_idx\"] = list(new_batch[\"batch_idx\"])\n",
    "        for i in range(len(new_batch[\"batch_idx\"])):\n",
    "            new_batch[\"batch_idx\"][i] += i  # add target image index for build_targets()\n",
    "        new_batch[\"batch_idx\"] = torch.cat(new_batch[\"batch_idx\"], 0)\n",
    "        return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae322fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.cfg import IterableSimpleNamespace\n",
    "from typing import Any, Dict\n",
    "from ultralytics.utils import colorstr\n",
    "\n",
    "def build_yolo_dataset(\n",
    "    cfg: IterableSimpleNamespace,\n",
    "    img_path: str,\n",
    "    batch: int,\n",
    "    data: Dict[str, Any],\n",
    "    mode: str = \"train\",\n",
    "    rect: bool = False,\n",
    "    stride: int = 32,\n",
    "    multi_modal: bool = False,\n",
    "):\n",
    "    \n",
    "    \"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\n",
    "    dataset = CustomYOLODataset # YOLOMultiModalDataset if multi_modal else CustomYOLODataset\n",
    "    return dataset(\n",
    "        img_path=img_path,\n",
    "        imgsz=cfg.imgsz,\n",
    "        batch_size=batch,\n",
    "        augment=mode == \"train\",  # augmentation\n",
    "        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function\n",
    "        rect=cfg.rect or rect,  # rectangular batches\n",
    "        cache=cfg.cache or None,\n",
    "        single_cls=cfg.single_cls or False,\n",
    "        stride=int(stride),\n",
    "        pad=0.0 if mode == \"train\" else 0.5,\n",
    "        prefix=colorstr(f\"{mode}: \"),\n",
    "        task=cfg.task,\n",
    "        classes=cfg.classes,\n",
    "        data=data,\n",
    "        fraction=cfg.fraction if mode == \"train\" else 1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73379303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics.nn.tasks import DetectionModel\n",
    "from ultralytics.utils.torch_utils import de_parallel\n",
    "from ultralytics.engine.trainer import BaseTrainer\n",
    "from ultralytics.utils import RANK\n",
    "from typing import Optional\n",
    "\n",
    "class CustomDetectionTrainer(DetectionTrainer):\n",
    "    \"\"\"ORIGINALLY INHERITS FROM THE DetectionTrainer, WHICH ORIGINALLY INHERITS FROM THE BaseTrainer\"\"\"\n",
    "    def build_dataset(self, img_path: str, mode: str = \"train\", batch: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Build YOLO Dataset for training or validation.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): Path to the folder containing images.\n",
    "            mode (str): 'train' mode or 'val' mode, users are able to customize different augmentations for each mode.\n",
    "            batch (int, optional): Size of batches, this is for 'rect' mode.\n",
    "\n",
    "        Returns:\n",
    "            (Dataset): YOLO dataset object configured for the specified mode.\n",
    "        \"\"\"\n",
    "        gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)\n",
    "        return build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, rect=mode == \"val\", stride=gs)\n",
    "\n",
    "    \"\"\"NO LONGER NEEDED TECHNICALLY\"\"\"\n",
    "    def get_model(self, cfg: Optional[str] = None, weights: Optional[str] = None, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Return a YOLO detection model.\n",
    "\n",
    "        Args:\n",
    "            cfg (str, optional): Path to model configuration file.\n",
    "            weights (str, optional): Path to model weights.\n",
    "            verbose (bool): Whether to display model information.\n",
    "\n",
    "        Returns:\n",
    "            (DetectionModel): YOLO detection model.\n",
    "        \"\"\"\n",
    "        print(cfg)\n",
    "        model = DetectionModel(cfg, nc=self.data[\"nc\"], ch=self.data[\"channels\"], verbose=verbose and RANK == -1)\n",
    "        if weights:\n",
    "            model.load(weights)\n",
    "        return model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e589cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.153 🚀 Python-3.12.11 torch-2.7.1+cu128 CUDA:0 (NVIDIA RTX A4000, 14967MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=512, bgr=0.0, box=6.5, cache=False, cfg=None, classes=None, close_mosaic=0, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=datasets/data.yaml, degrees=0.1, deterministic=True, device=None, dfl=2.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.5, format=torchscript, fraction=1.0, freeze=0, half=False, hsv_h=0.0, hsv_s=0.0, hsv_v=0.0, imgsz=192, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=1e-05, lrf=1e-05, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=0.0, multi_scale=False, name=yolo11_4ch_data, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.001, plots=True, pose=12.0, pretrained=False, profile=False, project=train_yolo11_4ch_2025_08_08_21_44_24, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=train_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.25, seed=42, shear=1, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding class names with single class.\n",
      "{'nc': 80, 'scales': {'n': [0.5, 0.25, 1024], 's': [0.5, 0.5, 1024], 'm': [0.5, 1.0, 512], 'l': [1.0, 1.0, 512], 'x': [1.0, 1.5, 512]}, 'backbone': [[-1, 1, 'Conv', [64, 3, 2]], [-1, 1, 'Conv', [128, 3, 2]], [-1, 2, 'C3k2', [256, False, 0.25]], [-1, 1, 'Conv', [256, 3, 2]], [-1, 2, 'C3k2', [512, False, 0.25]], [-1, 1, 'Conv', [512, 3, 2]], [-1, 2, 'C3k2', [512, True]], [-1, 1, 'Conv', [1024, 3, 2]], [-1, 2, 'C3k2', [1024, True]], [-1, 1, 'SPPF', [1024, 5]], [-1, 2, 'C2PSA', [1024]]], 'head': [[-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 6], 1, 'Concat', [1]], [-1, 2, 'C3k2', [512, False]], [-1, 1, 'nn.Upsample', ['None', 2, 'nearest']], [[-1, 4], 1, 'Concat', [1]], [-1, 2, 'C3k2', [256, False]], [-1, 1, 'Conv', [256, 3, 2]], [[-1, 13], 1, 'Concat', [1]], [-1, 2, 'C3k2', [512, False]], [-1, 1, 'Conv', [512, 3, 2]], [[-1, 10], 1, 'Concat', [1]], [-1, 2, 'C3k2', [1024, True]], [[16, 19, 22], 1, 'Detect', ['nc']]], 'scale': 's', 'yaml_file': 'yolo11s.yaml', 'ch': 3}\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1216  ultralytics.nn.modules.conv.Conv             [4, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLO11s summary: 181 layers, 9,428,467 parameters, 9,428,451 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 23.2±15.3 MB/s, size: 31.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/labels/train.cache... 6510 images, 3242 backgrounds, 0 corrupt: 100%|██████████| 6510/6510 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 29.6±22.3 MB/s, size: 27.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/labels/val.cache... 1395 images, 848 backgrounds, 0 corrupt: 100%|██████████| 1395/1395 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to train_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=1e-05' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.004), 87 bias(decay=0.0)\n",
      "Image sizes 192 train, 192 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mtrain_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5      11.3G      2.139      7.723      3.754        193        192: 100%|██████████| 13/13 [00:14<00:00,  1.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547    0.00146      0.627    0.00132   0.000528\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5      10.7G      1.191      2.444      2.205        173        192: 100%|██████████| 13/13 [00:07<00:00,  1.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.614      0.382      0.373      0.183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5      10.7G     0.8475     0.7463      1.663        188        192: 100%|██████████| 13/13 [00:07<00:00,  1.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547     0.0126     0.0768    0.00447    0.00108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5      10.7G     0.7668     0.6004      1.604        192        192: 100%|██████████| 13/13 [00:07<00:00,  1.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547    0.00647      0.199    0.00359    0.00114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5      10.7G     0.7071     0.5262      1.543        184        192: 100%|██████████| 13/13 [00:07<00:00,  1.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:03<00:00,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.762      0.464      0.496      0.324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from train_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data/weights/last.pt, 19.1MB\n",
      "Optimizer stripped from train_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data/weights/best.pt, 19.1MB\n",
      "\n",
      "Validating train_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data/weights/best.pt...\n",
      "Ultralytics 8.3.153 🚀 Python-3.12.11 torch-2.7.1+cu128 CUDA:0 (NVIDIA RTX A4000, 14967MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,475 parameters, 0 gradients, 21.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:04<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.763      0.465      0.497      0.324\n",
      "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mtrain_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from parameters import *\n",
    "\n",
    "def GetCurrentTime(): \n",
    "    current_time = time.localtime()\n",
    "    return time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "\n",
    "args = dict(model=\"yolo11s.pt\", \n",
    "            data=\"datasets/data.yaml\", \n",
    "            epochs=EPOCH, \n",
    "            pretrained=PRETRAINED, \n",
    "            imgsz=IMAGE_SIZE, \n",
    "            single_cls=SINGLE_CLS, \n",
    "            close_mosaic=CLOSE_MOSAIC, \n",
    "            fraction=FRACTION,\n",
    "            freeze=FREEZE,  \n",
    "            lr0=INITIAL_LR, \n",
    "            lrf=FINAL_LR, \n",
    "            warmup_epochs=WARMUP_EPOCH, \n",
    "            cls=CLS, \n",
    "            box=BOX, \n",
    "            dfl=DFL, \n",
    "            seed=SEED, \n",
    "            batch=BATCH,\n",
    "            amp=MIX_PRECISION, \n",
    "            multi_scale=MULTI_SCALE, \n",
    "            cos_lr=COS_LR,\n",
    "            plots=PLOT,\n",
    "            profile=PROFILE,\n",
    "            project=f\"{MODE}_{MODEL}_{GetCurrentTime()}\",\n",
    "            name=f\"{MODEL}_{DATASET}\", \n",
    "            \n",
    "            # Data Augmentation\n",
    "            hsv_h=HSV_H, \n",
    "            hsv_s=HSV_S, \n",
    "            hsv_v=HSV_V, \n",
    "            degrees=DEGREES,\n",
    "            translate=TRANSLATE,\n",
    "            scale=SCALE,\n",
    "            flipud=FLIPUD, \n",
    "            fliplr=FLIPLR, \n",
    "            mosaic=MOSAIC, \n",
    "            shear=SHEAR, \n",
    "            perspective=PERSPECTIVE, \n",
    "            mixup=MIXUP, \n",
    "            cutmix=CUTMIX)\n",
    "trainer = CustomDetectionTrainer(overrides=args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a7e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(trainer.data[\"channels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6dd00d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task=detect\n",
      "mode=train\n",
      "model=yolo11s.pt\n",
      "data=datasets/data.yaml\n",
      "epochs=5\n",
      "time=None\n",
      "patience=100\n",
      "batch=512\n",
      "imgsz=192\n",
      "save=True\n",
      "save_period=-1\n",
      "cache=False\n",
      "device=None\n",
      "workers=8\n",
      "project=train_yolo11_4ch_2025_08_08_21_44_24\n",
      "name=yolo11_4ch_data\n",
      "exist_ok=False\n",
      "pretrained=False\n",
      "optimizer=auto\n",
      "verbose=True\n",
      "seed=42\n",
      "deterministic=True\n",
      "single_cls=True\n",
      "rect=False\n",
      "cos_lr=True\n",
      "close_mosaic=0\n",
      "resume=False\n",
      "amp=True\n",
      "fraction=1.0\n",
      "profile=False\n",
      "freeze=0\n",
      "multi_scale=False\n",
      "overlap_mask=True\n",
      "mask_ratio=4\n",
      "dropout=0.0\n",
      "val=True\n",
      "split=val\n",
      "save_json=False\n",
      "conf=None\n",
      "iou=0.7\n",
      "max_det=300\n",
      "half=False\n",
      "dnn=False\n",
      "plots=True\n",
      "source=None\n",
      "vid_stride=1\n",
      "stream_buffer=False\n",
      "visualize=False\n",
      "augment=False\n",
      "agnostic_nms=False\n",
      "classes=None\n",
      "retina_masks=False\n",
      "embed=None\n",
      "show=False\n",
      "save_frames=False\n",
      "save_txt=False\n",
      "save_conf=False\n",
      "save_crop=False\n",
      "show_labels=True\n",
      "show_conf=True\n",
      "show_boxes=True\n",
      "line_width=None\n",
      "format=torchscript\n",
      "keras=False\n",
      "optimize=False\n",
      "int8=False\n",
      "dynamic=False\n",
      "simplify=True\n",
      "opset=None\n",
      "workspace=None\n",
      "nms=False\n",
      "lr0=1e-05\n",
      "lrf=1e-05\n",
      "momentum=0.937\n",
      "weight_decay=0.0005\n",
      "warmup_epochs=3\n",
      "warmup_momentum=0.8\n",
      "warmup_bias_lr=0.0\n",
      "box=6.5\n",
      "cls=0.5\n",
      "dfl=2.5\n",
      "pose=12.0\n",
      "kobj=1.0\n",
      "nbs=64\n",
      "hsv_h=0.0\n",
      "hsv_s=0.0\n",
      "hsv_v=0.0\n",
      "degrees=0.1\n",
      "translate=0.1\n",
      "scale=0.25\n",
      "shear=1\n",
      "perspective=0.001\n",
      "flipud=0.5\n",
      "fliplr=0.5\n",
      "bgr=0.0\n",
      "mosaic=0.0\n",
      "mixup=0.0\n",
      "cutmix=0.0\n",
      "copy_paste=0.0\n",
      "copy_paste_mode=flip\n",
      "auto_augment=randaugment\n",
      "erasing=0.4\n",
      "cfg=None\n",
      "tracker=botsort.yaml\n",
      "save_dir=train_yolo11_4ch_2025_08_08_21_44_24/yolo11_4ch_data\n"
     ]
    }
   ],
   "source": [
    "print(trainer.args)\n",
    "\n",
    "model = trainer.setup_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73bf0a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ultralytics.models.yolo.detect.val.DetectionValidator'>\n",
      "<ultralytics.models.yolo.detect.val.DetectionValidator object at 0x709644604560>\n"
     ]
    }
   ],
   "source": [
    "validator = trainer.get_validator()\n",
    "\n",
    "print(type(validator))\n",
    "print(validator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42459016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "MODEL_DIR = \"./train_yolo11_4ch_2025_08_04_17_18_27/yolo11_4ch_data/weights/best.pt\"\n",
    "\n",
    "print(os.path.exists(MODEL_DIR))\n",
    "\n",
    "model = YOLO(MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "359d0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO(\n",
      "  (model): DetectionModel(\n",
      "    (model): Sequential(\n",
      "      (0): Conv(\n",
      "        (conv): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (2): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (4): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (6): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): Conv(\n",
      "        (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (8): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): SPPF(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "      )\n",
      "      (10): C2PSA(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): Sequential(\n",
      "          (0): PSABlock(\n",
      "            (attn): Attention(\n",
      "              (qkv): Conv(\n",
      "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (proj): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "              (pe): Conv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "            (ffn): Sequential(\n",
      "              (0): Conv(\n",
      "                (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (12): Concat()\n",
      "      (13): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): Upsample(scale_factor=2.0, mode='nearest')\n",
      "      (15): Concat()\n",
      "      (16): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (18): Concat()\n",
      "      (19): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): Bottleneck(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (21): Concat()\n",
      "      (22): C3k2(\n",
      "        (cv1): Conv(\n",
      "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (cv2): Conv(\n",
      "          (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "          (act): SiLU(inplace=True)\n",
      "        )\n",
      "        (m): ModuleList(\n",
      "          (0): C3k(\n",
      "            (cv1): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv2): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (cv3): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (m): Sequential(\n",
      "              (0): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "              (1): Bottleneck(\n",
      "                (cv1): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "                (cv2): Conv(\n",
      "                  (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                  (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                  (act): SiLU(inplace=True)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): Detect(\n",
      "        (cv2): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (cv3): ModuleList(\n",
      "          (0): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "                (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "          (2): Sequential(\n",
      "            (0): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "                (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): DWConv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "          )\n",
      "        )\n",
      "        (dfl): DFL(\n",
      "          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b92dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.utils import FORMATS_HELP_MSG, IMG_FORMATS, VID_FORMATS\n",
    "from ultralytics.utils.checks import check_requirements\n",
    "from ultralytics.utils.patches import imread\n",
    "from typing import List, Tuple, Union\n",
    "from ultralytics.utils import LOGGER\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class CustomLoadImagesAndVideos:\n",
    "    \"\"\"\n",
    "    A class for loading and processing images and videos for YOLO object detection.\n",
    "\n",
    "    This class manages the loading and pre-processing of image and video data from various sources, including\n",
    "    single image files, video files, and lists of image and video paths.\n",
    "\n",
    "    Attributes:\n",
    "        files (List[str]): List of image and video file paths.\n",
    "        nf (int): Total number of files (images and videos).\n",
    "        video_flag (List[bool]): Flags indicating whether a file is a video (True) or an image (False).\n",
    "        mode (str): Current mode, 'image' or 'video'.\n",
    "        vid_stride (int): Stride for video frame-rate.\n",
    "        bs (int): Batch size.\n",
    "        cap (cv2.VideoCapture): Video capture object for OpenCV.\n",
    "        frame (int): Frame counter for video.\n",
    "        frames (int): Total number of frames in the video.\n",
    "        count (int): Counter for iteration, initialized at 0 during __iter__().\n",
    "        ni (int): Number of images.\n",
    "        cv2_flag (int): OpenCV flag for image reading (grayscale or RGB).\n",
    "\n",
    "    Methods:\n",
    "        __init__: Initialize the LoadImagesAndVideos object.\n",
    "        __iter__: Returns an iterator object for VideoStream or ImageFolder.\n",
    "        __next__: Returns the next batch of images or video frames along with their paths and metadata.\n",
    "        _new_video: Creates a new video capture object for the given path.\n",
    "        __len__: Returns the number of batches in the object.\n",
    "\n",
    "    Examples:\n",
    "        >>> loader = LoadImagesAndVideos(\"path/to/data\", batch=32, vid_stride=1)\n",
    "        >>> for paths, imgs, info in loader:\n",
    "        ...     # Process batch of images or video frames\n",
    "        ...     pass\n",
    "\n",
    "    Notes:\n",
    "        - Supports various image formats including HEIC.\n",
    "        - Handles both local files and directories.\n",
    "        - Can read from a text file containing paths to images and videos.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: Union[str, Path, List], batch: int = 1, vid_stride: int = 1, channels: int = 3):\n",
    "        \"\"\"\n",
    "        Initialize dataloader for images and videos, supporting various input formats.\n",
    "\n",
    "        Args:\n",
    "            path (str | Path | List): Path to images/videos, directory, or list of paths.\n",
    "            batch (int): Batch size for processing.\n",
    "            vid_stride (int): Video frame-rate stride.\n",
    "            channels (int): Number of image channels (1 for grayscale, 3 for RGB).\n",
    "        \"\"\"\n",
    "        parent = None\n",
    "        if isinstance(path, str) and Path(path).suffix == \".txt\":  # *.txt file with img/vid/dir on each line\n",
    "            parent = Path(path).parent\n",
    "            path = Path(path).read_text().splitlines()  # list of sources\n",
    "        files = []\n",
    "        for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\n",
    "            a = str(Path(p).absolute())  # do not use .resolve() https://github.com/ultralytics/ultralytics/issues/2912\n",
    "            if \"*\" in a:\n",
    "                files.extend(sorted(glob.glob(a, recursive=True)))  # glob\n",
    "            elif os.path.isdir(a):\n",
    "                files.extend(sorted(glob.glob(os.path.join(a, \"*.*\"))))  # dir\n",
    "            elif os.path.isfile(a):\n",
    "                files.append(a)  # files (absolute or relative to CWD)\n",
    "            elif parent and (parent / p).is_file():\n",
    "                files.append(str((parent / p).absolute()))  # files (relative to *.txt file parent)\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"{p} does not exist\")\n",
    "\n",
    "        # Define files as images or videos\n",
    "        images, videos = [], []\n",
    "        for f in files:\n",
    "            suffix = f.rpartition(\".\")[-1].lower()  # Get file extension without the dot and lowercase\n",
    "            if suffix in IMG_FORMATS:\n",
    "                images.append(f)\n",
    "            elif suffix in VID_FORMATS:\n",
    "                videos.append(f)\n",
    "        ni, nv = len(images), len(videos)\n",
    "\n",
    "        self.files = images + videos\n",
    "        self.nf = ni + nv  # number of files\n",
    "        self.ni = ni  # number of images\n",
    "        self.video_flag = [False] * ni + [True] * nv\n",
    "        self.mode = \"video\" if ni == 0 else \"image\"  # default to video if no images\n",
    "        self.vid_stride = vid_stride  # video frame-rate stride\n",
    "        self.bs = batch\n",
    "        self.cv2_flag = cv2.IMREAD_GRAYSCALE if channels == 1 else cv2.IMREAD_UNCHANGED  # grayscale or RGB\n",
    "        if any(videos):\n",
    "            self._new_video(videos[0])  # new video\n",
    "        else:\n",
    "            self.cap = None\n",
    "        if self.nf == 0:\n",
    "            raise FileNotFoundError(f\"No images or videos found in {p}. {FORMATS_HELP_MSG}\")\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate through image/video files, yielding source paths, images, and metadata.\"\"\"\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> Tuple[List[str], List[np.ndarray], List[str]]:\n",
    "        \"\"\"Return the next batch of images or video frames with their paths and metadata.\"\"\"\n",
    "        paths, imgs, info = [], [], []\n",
    "        while len(imgs) < self.bs:\n",
    "            if self.count >= self.nf:  # end of file list\n",
    "                if imgs:\n",
    "                    return paths, imgs, info  # return last partial batch\n",
    "                else:\n",
    "                    raise StopIteration\n",
    "\n",
    "            path = self.files[self.count]\n",
    "            if self.video_flag[self.count]:\n",
    "                self.mode = \"video\"\n",
    "                if not self.cap or not self.cap.isOpened():\n",
    "                    self._new_video(path)\n",
    "\n",
    "                success = False\n",
    "                for _ in range(self.vid_stride):\n",
    "                    success = self.cap.grab()\n",
    "                    if not success:\n",
    "                        break  # end of video or failure\n",
    "\n",
    "                if success:\n",
    "                    success, im0 = self.cap.retrieve()\n",
    "                    im0 = (\n",
    "                        cv2.cvtColor(im0, cv2.COLOR_BGR2GRAY)[..., None]\n",
    "                        if self.cv2_flag == cv2.IMREAD_GRAYSCALE\n",
    "                        else im0\n",
    "                    )\n",
    "                    if success:\n",
    "                        self.frame += 1\n",
    "                        paths.append(path)\n",
    "                        imgs.append(im0)\n",
    "                        info.append(f\"video {self.count + 1}/{self.nf} (frame {self.frame}/{self.frames}) {path}: \")\n",
    "                        if self.frame == self.frames:  # end of video\n",
    "                            self.count += 1\n",
    "                            self.cap.release()\n",
    "                else:\n",
    "                    # Move to the next file if the current video ended or failed to open\n",
    "                    self.count += 1\n",
    "                    if self.cap:\n",
    "                        self.cap.release()\n",
    "                    if self.count < self.nf:\n",
    "                        self._new_video(self.files[self.count])\n",
    "            else:\n",
    "                # Handle image files (including HEIC)\n",
    "                self.mode = \"image\"\n",
    "                if path.rpartition(\".\")[-1].lower() == \"heic\":\n",
    "                    # Load HEIC image using Pillow with pillow-heif\n",
    "                    check_requirements(\"pi-heif\")\n",
    "\n",
    "                    from pi_heif import register_heif_opener\n",
    "\n",
    "                    register_heif_opener()  # Register HEIF opener with Pillow\n",
    "                    with Image.open(path) as img:\n",
    "                        im0 = cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)  # convert image to BGR nparray\n",
    "                else:\n",
    "                    im0 = imread(path, flags=self.cv2_flag)  # BGR\n",
    "                if im0 is None:\n",
    "                    LOGGER.warning(f\"Image Read Error {path}\")\n",
    "                else:\n",
    "                    paths.append(path)\n",
    "                    imgs.append(im0)\n",
    "                    info.append(f\"image {self.count + 1}/{self.nf} {path}: \")\n",
    "                self.count += 1  # move to the next file\n",
    "                if self.count >= self.ni:  # end of image list\n",
    "                    break\n",
    "\n",
    "        return paths, imgs, info\n",
    "    \n",
    "    def _new_video(self, path: str):\n",
    "        \"\"\"Create a new video capture object for the given path and initialize video-related attributes.\"\"\"\n",
    "        self.frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
    "        if not self.cap.isOpened():\n",
    "            raise FileNotFoundError(f\"Failed to open video {path}\")\n",
    "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of files (images and videos) in the dataset.\"\"\"\n",
    "        return math.ceil(self.nf / self.bs)  # number of batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.data.build import check_source\n",
    "from ultralytics.data.loaders import (\n",
    "    LoadPilAndNumpy,\n",
    "    LoadScreenshots,\n",
    "    LoadStreams,\n",
    "    LoadTensor,\n",
    "    SourceTypes,\n",
    ")\n",
    "\n",
    "def load_inference_source(source=None, batch: int = 1, vid_stride: int = 1, buffer: bool = False, channels: int = 3):\n",
    "    \"\"\"\n",
    "    Load an inference source for object detection and apply necessary transformations.\n",
    "\n",
    "    Args:\n",
    "        source (str | Path | torch.Tensor | PIL.Image | np.ndarray, optional): The input source for inference.\n",
    "        batch (int, optional): Batch size for dataloaders.\n",
    "        vid_stride (int, optional): The frame interval for video sources.\n",
    "        buffer (bool, optional): Whether stream frames will be buffered.\n",
    "        channels (int, optional): The number of input channels for the model.\n",
    "\n",
    "    Returns:\n",
    "        (Dataset): A dataset object for the specified input source with attached source_type attribute.\n",
    "\n",
    "    Examples:\n",
    "        Load an image source for inference\n",
    "        >>> dataset = load_inference_source(\"image.jpg\", batch=1)\n",
    "\n",
    "        Load a video stream source\n",
    "        >>> dataset = load_inference_source(\"rtsp://example.com/stream\", vid_stride=2)\n",
    "    \"\"\" \n",
    "    \n",
    "    source, stream, screenshot, from_img, in_memory, tensor = check_source(source)\n",
    "    source_type = source.source_type if in_memory else SourceTypes(stream, screenshot, from_img, tensor)\n",
    "\n",
    "    print(f\"\\n\\nThis is channel again {channels}\")\n",
    "    print(f\"\\n\\nThis is source type {source_type}\")\n",
    "\n",
    "    # Dataloader\n",
    "    if tensor:\n",
    "        dataset = LoadTensor(source)\n",
    "    elif in_memory:\n",
    "        dataset = source\n",
    "    elif stream:\n",
    "        dataset = LoadStreams(source, vid_stride=vid_stride, buffer=buffer, channels=channels)\n",
    "    elif screenshot:\n",
    "        dataset = LoadScreenshots(source, channels=channels)\n",
    "    elif from_img:\n",
    "        dataset = LoadPilAndNumpy(source, channels=channels)\n",
    "    else:\n",
    "        dataset = CustomLoadImagesAndVideos(source, batch=batch, vid_stride=vid_stride, channels=channels)\n",
    "\n",
    "    # Attach source types to the dataset\n",
    "    setattr(dataset, \"source_type\", source_type)\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7984c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.predictor import BasePredictor\n",
    "from ultralytics.utils.checks import check_imgsz\n",
    "\n",
    "STREAM_WARNING = \"\"\"\n",
    "inference results will accumulate in RAM unless `stream=True` is passed, causing potential out-of-memory\n",
    "errors for large sources or long-running streams and videos. See https://docs.ultralytics.com/modes/predict/ for help.\n",
    "\n",
    "Example:\n",
    "    results = model(source=..., stream=True)  # generator of Results objects\n",
    "    for r in results:\n",
    "        boxes = r.boxes  # Boxes object for bbox outputs\n",
    "        masks = r.masks  # Masks object for segment masks outputs\n",
    "        probs = r.probs  # Class probabilities for classification outputs\n",
    "\"\"\"\n",
    "\n",
    "class CustomBasePredictor(BasePredictor): \n",
    "    def setup_source(self, source): \n",
    "        \"\"\"\n",
    "        Set up source and inference mode.\n",
    "\n",
    "        Args:\n",
    "            source (str | Path | List[str] | List[Path] | List[np.ndarray] | np.ndarray | torch.Tensor):\n",
    "                Source for inference.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"\\nTHIS IS NEW\")\n",
    "\n",
    "        print(f\"\\n\\nThis is {getattr(self.model, \"ch\", 4),}\")\n",
    "\n",
    "        self.imgsz = check_imgsz(self.args.imgsz, stride=self.model.stride, min_dim=2)  # check image size\n",
    "        self.dataset = load_inference_source(\n",
    "            source=source,\n",
    "            batch=self.args.batch,\n",
    "            vid_stride=self.args.vid_stride,\n",
    "            buffer=self.args.stream_buffer,\n",
    "            channels=getattr(self.model, \"ch\", 4),\n",
    "        )\n",
    "        self.source_type = self.dataset.source_type\n",
    "        if not getattr(self, \"stream\", True) and (\n",
    "            self.source_type.stream\n",
    "            or self.source_type.screenshot\n",
    "            or len(self.dataset) > 1000  # many images\n",
    "            or any(getattr(self.dataset, \"video_flag\", [False]))\n",
    "        ):  # videos\n",
    "            LOGGER.warning(STREAM_WARNING)\n",
    "        self.vid_writer = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4d688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.engine.results import Results\n",
    "from ultralytics.utils import ops\n",
    "\n",
    "class CustomDetectionPredictor(CustomBasePredictor):\n",
    "    \"\"\"\n",
    "    A class extending the BasePredictor class for prediction based on a detection model.\n",
    "\n",
    "    This predictor specializes in object detection tasks, processing model outputs into meaningful detection results\n",
    "    with bounding boxes and class predictions.\n",
    "\n",
    "    Attributes:\n",
    "        args (namespace): Configuration arguments for the predictor.\n",
    "        model (nn.Module): The detection model used for inference.\n",
    "        batch (list): Batch of images and metadata for processing.\n",
    "\n",
    "    Methods:\n",
    "        postprocess: Process raw model predictions into detection results.\n",
    "        construct_results: Build Results objects from processed predictions.\n",
    "        construct_result: Create a single Result object from a prediction.\n",
    "        get_obj_feats: Extract object features from the feature maps.\n",
    "\n",
    "    Examples:\n",
    "        >>> from ultralytics.utils import ASSETS\n",
    "        >>> from ultralytics.models.yolo.detect import DetectionPredictor\n",
    "        >>> args = dict(model=\"yolo11n.pt\", source=ASSETS)\n",
    "        >>> predictor = DetectionPredictor(overrides=args)\n",
    "        >>> predictor.predict_cli()\n",
    "    \"\"\"\n",
    "\n",
    "    def postprocess(self, preds, img, orig_imgs, **kwargs):\n",
    "        \"\"\"\n",
    "        Post-process predictions and return a list of Results objects.\n",
    "\n",
    "        This method applies non-maximum suppression to raw model predictions and prepares them for visualization and\n",
    "        further analysis.\n",
    "\n",
    "        Args:\n",
    "            preds (torch.Tensor): Raw predictions from the model.\n",
    "            img (torch.Tensor): Processed input image tensor in model input format.\n",
    "            orig_imgs (torch.Tensor | list): Original input images before preprocessing.\n",
    "            **kwargs (Any): Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "            (list): List of Results objects containing the post-processed predictions.\n",
    "\n",
    "        Examples:\n",
    "            >>> predictor = DetectionPredictor(overrides=dict(model=\"yolo11n.pt\"))\n",
    "            >>> results = predictor.predict(\"path/to/image.jpg\")\n",
    "            >>> processed_results = predictor.postprocess(preds, img, orig_imgs)\n",
    "        \"\"\"\n",
    "        save_feats = getattr(self, \"_feats\", None) is not None\n",
    "        preds = ops.non_max_suppression(\n",
    "            preds,\n",
    "            self.args.conf,\n",
    "            self.args.iou,\n",
    "            self.args.classes,\n",
    "            self.args.agnostic_nms,\n",
    "            max_det=self.args.max_det,\n",
    "            nc=0 if self.args.task == \"detect\" else len(self.model.names),\n",
    "            end2end=getattr(self.model, \"end2end\", False),\n",
    "            rotated=self.args.task == \"obb\",\n",
    "            return_idxs=save_feats,\n",
    "        )\n",
    "\n",
    "        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list\n",
    "            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
    "\n",
    "        if save_feats:\n",
    "            obj_feats = self.get_obj_feats(self._feats, preds[1])\n",
    "            preds = preds[0]\n",
    "\n",
    "        results = self.construct_results(preds, img, orig_imgs, **kwargs)\n",
    "\n",
    "        if save_feats:\n",
    "            for r, f in zip(results, obj_feats):\n",
    "                r.feats = f  # add object features to results\n",
    "\n",
    "        return results\n",
    "\n",
    "    def get_obj_feats(self, feat_maps, idxs):\n",
    "        \"\"\"Extract object features from the feature maps.\"\"\"\n",
    "        import torch\n",
    "\n",
    "        s = min([x.shape[1] for x in feat_maps])  # find smallest vector length\n",
    "        obj_feats = torch.cat(\n",
    "            [x.permute(0, 2, 3, 1).reshape(x.shape[0], -1, s, x.shape[1] // s).mean(dim=-1) for x in feat_maps], dim=1\n",
    "        )  # mean reduce all vectors to same length\n",
    "        return [feats[idx] if len(idx) else [] for feats, idx in zip(obj_feats, idxs)]  # for each img in batch\n",
    "\n",
    "    def construct_results(self, preds, img, orig_imgs):\n",
    "        \"\"\"\n",
    "        Construct a list of Results objects from model predictions.\n",
    "\n",
    "        Args:\n",
    "            preds (List[torch.Tensor]): List of predicted bounding boxes and scores for each image.\n",
    "            img (torch.Tensor): Batch of preprocessed images used for inference.\n",
    "            orig_imgs (List[np.ndarray]): List of original images before preprocessing.\n",
    "\n",
    "        Returns:\n",
    "            (List[Results]): List of Results objects containing detection information for each image.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.construct_result(pred, img, orig_img, img_path)\n",
    "            for pred, orig_img, img_path in zip(preds, orig_imgs, self.batch[0])\n",
    "        ]\n",
    "\n",
    "    def construct_result(self, pred, img, orig_img, img_path):\n",
    "        \"\"\"\n",
    "        Construct a single Results object from one image prediction.\n",
    "\n",
    "        Args:\n",
    "            pred (torch.Tensor): Predicted boxes and scores with shape (N, 6) where N is the number of detections.\n",
    "            img (torch.Tensor): Preprocessed image tensor used for inference.\n",
    "            orig_img (np.ndarray): Original image before preprocessing.\n",
    "            img_path (str): Path to the original image file.\n",
    "\n",
    "        Returns:\n",
    "            (Results): Results object containing the original image, image path, class names, and scaled bounding boxes.\n",
    "        \"\"\"\n",
    "        pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)\n",
    "        return Results(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd2e876f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.153 🚀 Python-3.12.11 torch-2.7.1+cu128 CUDA:0 (NVIDIA RTX A4000, 14967MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,475 parameters, 0 gradients, 21.4 GFLOPs\n",
      "\n",
      "\n",
      "THIS IS NEW\n",
      "\n",
      "\n",
      "This is (4,)\n",
      "\n",
      "\n",
      "This is channel again 4\n",
      "\n",
      "\n",
      "This is source type SourceTypes(stream=False, screenshot=False, from_img=False, tensor=False)\n",
      "image 1/1 /home/jun/Desktop/inspirit/yolo_unet/train_yolo/BraTS-SSA-00002-0000-t1c.png: 640x640 3 items, 8.4ms\n",
      "Speed: 2.4ms preprocess, 8.4ms inference, 248.9ms postprocess per image at shape (1, 4, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/train57\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/eog: symbol lookup error: /snap/core20/current/lib/x86_64-linux-gnu/libpthread.so.0: undefined symbol: __libc_pthread_init, version GLIBC_PRIVATE\n"
     ]
    }
   ],
   "source": [
    "### Trying Out Predictors\n",
    "\n",
    "predictor = CustomDetectionPredictor()\n",
    "\n",
    "predictor.setup_model(\"best.pt\")\n",
    "\n",
    "image = \"BraTS-SSA-00002-0000-t1c.png\"\n",
    "\n",
    "results = predictor(image)\n",
    "\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing Out Data Augmentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

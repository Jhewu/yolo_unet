{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554668a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from ultralytics.data.utils import FORMATS_HELP_MSG, HELP_URL, IMG_FORMATS, check_file_speeds\n",
    "from ultralytics.utils import DEFAULT_CFG, LOCAL_RANK, LOGGER, NUM_THREADS, TQDM\n",
    "from ultralytics.utils.patches import imread\n",
    "\n",
    "class CustomBaseDataset(Dataset):\n",
    "    \"\"\"ORIGINALLY INHERITS FROM THE Dataset CLASS (TORCH)\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_path: Union[str, List[str]],\n",
    "        imgsz: int = 640,\n",
    "        cache: Union[bool, str] = False,\n",
    "        augment: bool = True,\n",
    "        hyp: Dict[str, Any] = DEFAULT_CFG,\n",
    "        prefix: str = \"\",\n",
    "        rect: bool = False,\n",
    "        batch_size: int = 16,\n",
    "        stride: int = 32,\n",
    "        pad: float = 0.5,\n",
    "        single_cls: bool = False,\n",
    "        classes: Optional[List[int]] = None,\n",
    "        fraction: float = 1.0,\n",
    "        channels: int = 4,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize BaseDataset with given configuration and options.\n",
    "\n",
    "        Args:\n",
    "            img_path (str | List[str]): Path to the folder containing images or list of image paths.\n",
    "            imgsz (int): Image size for resizing.\n",
    "            cache (bool | str): Cache images to RAM or disk during training.\n",
    "            augment (bool): If True, data augmentation is applied.\n",
    "            hyp (Dict[str, Any]): Hyperparameters to apply data augmentation.\n",
    "            prefix (str): Prefix to print in log messages.\n",
    "            rect (bool): If True, rectangular training is used.\n",
    "            batch_size (int): Size of batches.\n",
    "            stride (int): Stride used in the model.\n",
    "            pad (float): Padding value.\n",
    "            single_cls (bool): If True, single class training is used.\n",
    "            classes (List[int], optional): List of included classes.\n",
    "            fraction (float): Fraction of dataset to utilize.\n",
    "            channels (int): Number of channels in the images (1 for grayscale, 3 for RGB).\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.img_path = img_path\n",
    "        self.imgsz = imgsz\n",
    "        self.augment = augment\n",
    "        self.single_cls = single_cls\n",
    "        self.prefix = prefix\n",
    "        self.fraction = fraction\n",
    "        self.channels = channels\n",
    "        self.cv2_flag = cv2.IMREAD_GRAYSCALE if channels == 1 else cv2.IMREAD_UNCHANGED\n",
    "        self.im_files = self.get_img_files(self.img_path)\n",
    "        self.labels = self.get_labels()\n",
    "        self.update_labels(include_class=classes)  # single_cls and include_class\n",
    "        self.ni = len(self.labels)  # number of images\n",
    "        self.rect = rect\n",
    "        self.batch_size = batch_size\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        if self.rect:\n",
    "            assert self.batch_size is not None\n",
    "            self.set_rectangle()\n",
    "\n",
    "        # Buffer thread for mosaic images\n",
    "        self.buffer = []  # buffer size = batch size\n",
    "        self.max_buffer_length = min((self.ni, self.batch_size * 8, 1000)) if self.augment else 0\n",
    "\n",
    "        # Cache images (options are cache = True, False, None, \"ram\", \"disk\")\n",
    "        self.ims, self.im_hw0, self.im_hw = [None] * self.ni, [None] * self.ni, [None] * self.ni\n",
    "        self.npy_files = [Path(f).with_suffix(\".npy\") for f in self.im_files]\n",
    "        self.cache = cache.lower() if isinstance(cache, str) else \"ram\" if cache is True else None\n",
    "        if self.cache == \"ram\" and self.check_cache_ram():\n",
    "            if hyp.deterministic:\n",
    "                LOGGER.warning(\n",
    "                    \"cache='ram' may produce non-deterministic training results. \"\n",
    "                    \"Consider cache='disk' as a deterministic alternative if your disk space allows.\"\n",
    "                )\n",
    "            self.cache_images()\n",
    "        elif self.cache == \"disk\" and self.check_cache_disk():\n",
    "            self.cache_images()\n",
    "\n",
    "        # Transforms\n",
    "        self.transforms = self.build_transforms(hyp=hyp)\n",
    "\n",
    "    def get_img_files(self, img_path: Union[str, List[str]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Read image files from the specified path.\n",
    "\n",
    "        Args:\n",
    "            img_path (str | List[str]): Path or list of paths to image directories or files.\n",
    "\n",
    "        Returns:\n",
    "            (List[str]): List of image file paths.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If no images are found or the path doesn't exist.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            f = []  # image files\n",
    "            for p in img_path if isinstance(img_path, list) else [img_path]:\n",
    "                p = Path(p)  # os-agnostic\n",
    "                if p.is_dir():  # dir\n",
    "                    f += glob.glob(str(p / \"**\" / \"*.*\"), recursive=True)\n",
    "                    # F = list(p.rglob('*.*'))  # pathlib\n",
    "                elif p.is_file():  # file\n",
    "                    with open(p, encoding=\"utf-8\") as t:\n",
    "                        t = t.read().strip().splitlines()\n",
    "                        parent = str(p.parent) + os.sep\n",
    "                        f += [x.replace(\"./\", parent) if x.startswith(\"./\") else x for x in t]  # local to global path\n",
    "                        # F += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"{self.prefix}{p} does not exist\")\n",
    "            im_files = sorted(x.replace(\"/\", os.sep) for x in f if x.rpartition(\".\")[-1].lower() in IMG_FORMATS)\n",
    "            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\n",
    "            assert im_files, f\"{self.prefix}No images found in {img_path}. {FORMATS_HELP_MSG}\"\n",
    "        except Exception as e:\n",
    "            raise FileNotFoundError(f\"{self.prefix}Error loading data from {img_path}\\n{HELP_URL}\") from e\n",
    "        if self.fraction < 1:\n",
    "            im_files = im_files[: round(len(im_files) * self.fraction)]  # retain a fraction of the dataset\n",
    "        check_file_speeds(im_files, prefix=self.prefix)  # check image read speeds\n",
    "        return im_files\n",
    "\n",
    "    def update_labels(self, include_class: Optional[List[int]]) -> None:\n",
    "        \"\"\"\n",
    "        Update labels to include only specified classes.\n",
    "\n",
    "        Args:\n",
    "            include_class (List[int], optional): List of classes to include. If None, all classes are included.\n",
    "        \"\"\"\n",
    "        include_class_array = np.array(include_class).reshape(1, -1)\n",
    "        for i in range(len(self.labels)):\n",
    "            if include_class is not None:\n",
    "                cls = self.labels[i][\"cls\"]\n",
    "                bboxes = self.labels[i][\"bboxes\"]\n",
    "                segments = self.labels[i][\"segments\"]\n",
    "                keypoints = self.labels[i][\"keypoints\"]\n",
    "                j = (cls == include_class_array).any(1)\n",
    "                self.labels[i][\"cls\"] = cls[j]\n",
    "                self.labels[i][\"bboxes\"] = bboxes[j]\n",
    "                if segments:\n",
    "                    self.labels[i][\"segments\"] = [segments[si] for si, idx in enumerate(j) if idx]\n",
    "                if keypoints is not None:\n",
    "                    self.labels[i][\"keypoints\"] = keypoints[j]\n",
    "            if self.single_cls:\n",
    "                self.labels[i][\"cls\"][:, 0] = 0\n",
    "\n",
    "    def load_image(self, i: int, rect_mode: bool = True) -> Tuple[np.ndarray, Tuple[int, int], Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Load an image from dataset index 'i'.\n",
    "\n",
    "        Args:\n",
    "            i (int): Index of the image to load.\n",
    "            rect_mode (bool): Whether to use rectangular resizing.\n",
    "\n",
    "        Returns:\n",
    "            im (np.ndarray): Loaded image as a NumPy array.\n",
    "            hw_original (Tuple[int, int]): Original image dimensions in (height, width) format.\n",
    "            hw_resized (Tuple[int, int]): Resized image dimensions in (height, width) format.\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If the image file is not found.\n",
    "        \"\"\"\n",
    "        im, f, fn = self.ims[i], self.im_files[i], self.npy_files[i]\n",
    "        if im is None:  # not cached in RAM\n",
    "            if fn.exists():  # load npy\n",
    "                try:\n",
    "                    im = np.load(fn)\n",
    "                except Exception as e:\n",
    "                    LOGGER.warning(f\"{self.prefix}Removing corrupt *.npy image file {fn} due to: {e}\")\n",
    "                    Path(fn).unlink(missing_ok=True)\n",
    "                    im = imread(f, flags=self.cv2_flag)  # BGR\n",
    "            else:  # read image\n",
    "                im = imread(f, flags=self.cv2_flag)  # BGR\n",
    "            if im is None:\n",
    "                raise FileNotFoundError(f\"Image Not Found {f}\")\n",
    "\n",
    "            h0, w0 = im.shape[:2]  # orig hw\n",
    "            if rect_mode:  # resize long side to imgsz while maintaining aspect ratio\n",
    "                r = self.imgsz / max(h0, w0)  # ratio\n",
    "                if r != 1:  # if sizes are not equal\n",
    "                    w, h = (min(math.ceil(w0 * r), self.imgsz), min(math.ceil(h0 * r), self.imgsz))\n",
    "                    im = cv2.resize(im, (w, h), interpolation=cv2.INTER_LINEAR)\n",
    "            elif not (h0 == w0 == self.imgsz):  # resize by stretching image to square imgsz\n",
    "                im = cv2.resize(im, (self.imgsz, self.imgsz), interpolation=cv2.INTER_LINEAR)\n",
    "            if im.ndim == 2:\n",
    "                im = im[..., None]\n",
    "\n",
    "            # Add to buffer if training with augmentations\n",
    "            if self.augment:\n",
    "                self.ims[i], self.im_hw0[i], self.im_hw[i] = im, (h0, w0), im.shape[:2]  # im, hw_original, hw_resized\n",
    "                self.buffer.append(i)\n",
    "                if 1 < len(self.buffer) >= self.max_buffer_length:  # prevent empty buffer\n",
    "                    j = self.buffer.pop(0)\n",
    "                    if self.cache != \"ram\":\n",
    "                        self.ims[j], self.im_hw0[j], self.im_hw[j] = None, None, None\n",
    "\n",
    "            return im, (h0, w0), im.shape[:2]\n",
    "\n",
    "        return self.ims[i], self.im_hw0[i], self.im_hw[i]\n",
    "\n",
    "    def cache_images(self) -> None:\n",
    "        \"\"\"Cache images to memory or disk for faster training.\"\"\"\n",
    "        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes\n",
    "        fcn, storage = (self.cache_images_to_disk, \"Disk\") if self.cache == \"disk\" else (self.load_image, \"RAM\")\n",
    "        with ThreadPool(NUM_THREADS) as pool:\n",
    "            results = pool.imap(fcn, range(self.ni))\n",
    "            pbar = TQDM(enumerate(results), total=self.ni, disable=LOCAL_RANK > 0)\n",
    "            for i, x in pbar:\n",
    "                if self.cache == \"disk\":\n",
    "                    b += self.npy_files[i].stat().st_size\n",
    "                else:  # 'ram'\n",
    "                    self.ims[i], self.im_hw0[i], self.im_hw[i] = x  # im, hw_orig, hw_resized = load_image(self, i)\n",
    "                    b += self.ims[i].nbytes\n",
    "                pbar.desc = f\"{self.prefix}Caching images ({b / gb:.1f}GB {storage})\"\n",
    "            pbar.close()\n",
    "\n",
    "    def cache_images_to_disk(self, i: int) -> None:\n",
    "        \"\"\"Save an image as an *.npy file for faster loading.\"\"\"\n",
    "        f = self.npy_files[i]\n",
    "        if not f.exists():\n",
    "            np.save(f.as_posix(), imread(self.im_files[i]), allow_pickle=False)\n",
    "\n",
    "    def check_cache_disk(self, safety_margin: float = 0.5) -> bool:\n",
    "        \"\"\"\n",
    "        Check if there's enough disk space for caching images.\n",
    "\n",
    "        Args:\n",
    "            safety_margin (float): Safety margin factor for disk space calculation.\n",
    "\n",
    "        Returns:\n",
    "            (bool): True if there's enough disk space, False otherwise.\n",
    "        \"\"\"\n",
    "        import shutil\n",
    "\n",
    "        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes\n",
    "        n = min(self.ni, 30)  # extrapolate from 30 random images\n",
    "        for _ in range(n):\n",
    "            im_file = random.choice(self.im_files)\n",
    "            im = imread(im_file)\n",
    "            if im is None:\n",
    "                continue\n",
    "            b += im.nbytes\n",
    "            if not os.access(Path(im_file).parent, os.W_OK):\n",
    "                self.cache = None\n",
    "                LOGGER.warning(f\"{self.prefix}Skipping caching images to disk, directory not writeable\")\n",
    "                return False\n",
    "        disk_required = b * self.ni / n * (1 + safety_margin)  # bytes required to cache dataset to disk\n",
    "        total, used, free = shutil.disk_usage(Path(self.im_files[0]).parent)\n",
    "        if disk_required > free:\n",
    "            self.cache = None\n",
    "            LOGGER.warning(\n",
    "                f\"{self.prefix}{disk_required / gb:.1f}GB disk space required, \"\n",
    "                f\"with {int(safety_margin * 100)}% safety margin but only \"\n",
    "                f\"{free / gb:.1f}/{total / gb:.1f}GB free, not caching images to disk\"\n",
    "            )\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def check_cache_ram(self, safety_margin: float = 0.5) -> bool:\n",
    "        \"\"\"\n",
    "        Check if there's enough RAM for caching images.\n",
    "\n",
    "        Args:\n",
    "            safety_margin (float): Safety margin factor for RAM calculation.\n",
    "\n",
    "        Returns:\n",
    "            (bool): True if there's enough RAM, False otherwise.\n",
    "        \"\"\"\n",
    "        b, gb = 0, 1 << 30  # bytes of cached images, bytes per gigabytes\n",
    "        n = min(self.ni, 30)  # extrapolate from 30 random images\n",
    "        for _ in range(n):\n",
    "            im = imread(random.choice(self.im_files))  # sample image\n",
    "            if im is None:\n",
    "                continue\n",
    "            ratio = self.imgsz / max(im.shape[0], im.shape[1])  # max(h, w)  # ratio\n",
    "            b += im.nbytes * ratio**2\n",
    "        mem_required = b * self.ni / n * (1 + safety_margin)  # GB required to cache dataset into RAM\n",
    "        mem = __import__(\"psutil\").virtual_memory()\n",
    "        if mem_required > mem.available:\n",
    "            self.cache = None\n",
    "            LOGGER.warning(\n",
    "                f\"{self.prefix}{mem_required / gb:.1f}GB RAM required to cache images \"\n",
    "                f\"with {int(safety_margin * 100)}% safety margin but only \"\n",
    "                f\"{mem.available / gb:.1f}/{mem.total / gb:.1f}GB available, not caching images\"\n",
    "            )\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def set_rectangle(self) -> None:\n",
    "        \"\"\"Set the shape of bounding boxes for YOLO detections as rectangles.\"\"\"\n",
    "        bi = np.floor(np.arange(self.ni) / self.batch_size).astype(int)  # batch index\n",
    "        nb = bi[-1] + 1  # number of batches\n",
    "\n",
    "        s = np.array([x.pop(\"shape\") for x in self.labels])  # hw\n",
    "        ar = s[:, 0] / s[:, 1]  # aspect ratio\n",
    "        irect = ar.argsort()\n",
    "        self.im_files = [self.im_files[i] for i in irect]\n",
    "        self.labels = [self.labels[i] for i in irect]\n",
    "        ar = ar[irect]\n",
    "\n",
    "        # Set training image shapes\n",
    "        shapes = [[1, 1]] * nb\n",
    "        for i in range(nb):\n",
    "            ari = ar[bi == i]\n",
    "            mini, maxi = ari.min(), ari.max()\n",
    "            if maxi < 1:\n",
    "                shapes[i] = [maxi, 1]\n",
    "            elif mini > 1:\n",
    "                shapes[i] = [1, 1 / mini]\n",
    "\n",
    "        self.batch_shapes = np.ceil(np.array(shapes) * self.imgsz / self.stride + self.pad).astype(int) * self.stride\n",
    "        self.batch = bi  # batch index of image\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        \"\"\"Return transformed label information for given index.\"\"\"\n",
    "        return self.transforms(self.get_image_and_label(index))\n",
    "\n",
    "    def get_image_and_label(self, index: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get and return label information from the dataset.\n",
    "\n",
    "        Args:\n",
    "            index (int): Index of the image to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            (Dict[str, Any]): Label dictionary with image and metadata.\n",
    "        \"\"\"\n",
    "        label = deepcopy(self.labels[index])  # requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\n",
    "        label.pop(\"shape\", None)  # shape is for rect, remove it\n",
    "        label[\"img\"], label[\"ori_shape\"], label[\"resized_shape\"] = self.load_image(index)\n",
    "        label[\"ratio_pad\"] = (\n",
    "            label[\"resized_shape\"][0] / label[\"ori_shape\"][0],\n",
    "            label[\"resized_shape\"][1] / label[\"ori_shape\"][1],\n",
    "        )  # for evaluation\n",
    "        if self.rect:\n",
    "            label[\"rect_shape\"] = self.batch_shapes[self.batch[index]]\n",
    "        return self.update_labels_info(label)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the length of the labels list for the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def update_labels_info(self, label: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Custom your label format here.\"\"\"\n",
    "        return label\n",
    "\n",
    "    def build_transforms(self, hyp: Optional[Dict[str, Any]] = None):\n",
    "        \"\"\"\n",
    "        Users can customize augmentations here.\n",
    "\n",
    "        Examples:\n",
    "            >>> if self.augment:\n",
    "            ...     # Training transforms\n",
    "            ...     return Compose([])\n",
    "            >>> else:\n",
    "            ...    # Val transforms\n",
    "            ...    return Compose([])\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_labels(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Users can customize their own format here.\n",
    "\n",
    "        Examples:\n",
    "            Ensure output is a dictionary with the following keys:\n",
    "            >>> dict(\n",
    "            ...     im_file=im_file,\n",
    "            ...     shape=shape,  # format: (height, width)\n",
    "            ...     cls=cls,\n",
    "            ...     bboxes=bboxes,  # xywh\n",
    "            ...     segments=segments,  # xy\n",
    "            ...     keypoints=keypoints,  # xy\n",
    "            ...     normalized=True,  # or False\n",
    "            ...     bbox_format=\"xyxy\",  # or xywh, ltwh\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba39cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.utils import LOGGER, NUM_THREADS, LOCAL_RANK\n",
    "from ultralytics.data.utils import verify_image_label\n",
    "from ultralytics.utils.ops import resample_segments\n",
    "from ultralytics.data.dataset import YOLODataset\n",
    "from ultralytics.utils.instance import Instances\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from ultralytics.utils import TQDM \n",
    "from typing import Optional, Dict\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import torch \n",
    "\n",
    "from ultralytics.data.utils import (\n",
    "    HELP_URL, \n",
    "    get_hash, \n",
    "    save_dataset_cache_file, \n",
    "    img2label_paths, \n",
    "    load_dataset_cache_file\n",
    ")\n",
    "\n",
    "from ultralytics.data.augment import (\n",
    "    Compose, \n",
    "    Format,\n",
    "    LetterBox,\n",
    "    v8_transforms)\n",
    "\n",
    "DATASET_CACHE_VERSION = \"1.0.3\"\n",
    "\n",
    "class CustomYOLODataset(CustomBaseDataset):\n",
    "    \"\"\"\n",
    "    Dataset class for loading object detection and/or segmentation labels in YOLO format.\n",
    "\n",
    "    This class supports loading data for object detection, segmentation, pose estimation, and oriented bounding box\n",
    "    (OBB) tasks using the YOLO format.\n",
    "\n",
    "    Attributes:\n",
    "        use_segments (bool): Indicates if segmentation masks should be used.\n",
    "        use_keypoints (bool): Indicates if keypoints should be used for pose estimation.\n",
    "        use_obb (bool): Indicates if oriented bounding boxes should be used.\n",
    "        data (dict): Dataset configuration dictionary.\n",
    "\n",
    "    Methods:\n",
    "        cache_labels: Cache dataset labels, check images and read shapes.\n",
    "        get_labels: Return dictionary of labels for YOLO training.\n",
    "        build_transforms: Build and append transforms to the list.\n",
    "        close_mosaic: Set mosaic, copy_paste and mixup options to 0.0 and build transformations.\n",
    "        update_labels_info: Update label format for different tasks.\n",
    "        collate_fn: Collate data samples into batches.\n",
    "\n",
    "    Examples:\n",
    "        >>> dataset = YOLODataset(img_path=\"path/to/images\", data={\"names\": {0: \"person\"}}, task=\"detect\")\n",
    "        >>> dataset.get_labels()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, data: Optional[Dict] = None, task: str = \"detect\", **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the YOLODataset.\n",
    "\n",
    "        Args:\n",
    "            data (dict, optional): Dataset configuration dictionary.\n",
    "            task (str): Task type, one of 'detect', 'segment', 'pose', or 'obb'.\n",
    "            *args (Any): Additional positional arguments for the parent class.\n",
    "            **kwargs (Any): Additional keyword arguments for the parent class.\n",
    "        \"\"\"\n",
    "        self.use_segments = task == \"segment\"\n",
    "        self.use_keypoints = task == \"pose\"\n",
    "        self.use_obb = task == \"obb\"\n",
    "        self.data = data\n",
    "\n",
    "        assert not (self.use_segments and self.use_keypoints), \"Can not use both segments and keypoints.\"\n",
    "        super().__init__(*args, channels=self.data[\"channels\"], **kwargs)\n",
    "\n",
    "    def cache_labels(self, path: Path = Path(\"./labels.cache\")) -> Dict:\n",
    "        \"\"\"\n",
    "        Cache dataset labels, check images and read shapes.\n",
    "\n",
    "        Args:\n",
    "            path (Path): Path where to save the cache file.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Dictionary containing cached labels and related information.\n",
    "        \"\"\"\n",
    "        x = {\"labels\": []}\n",
    "        nm, nf, ne, nc, msgs = 0, 0, 0, 0, []  # number missing, found, empty, corrupt, messages\n",
    "        desc = f\"{self.prefix}Scanning {path.parent / path.stem}...\"\n",
    "        total = len(self.im_files)\n",
    "        nkpt, ndim = self.data.get(\"kpt_shape\", (0, 0))\n",
    "        if self.use_keypoints and (nkpt <= 0 or ndim not in {2, 3}):\n",
    "            raise ValueError(\n",
    "                \"'kpt_shape' in data.yaml missing or incorrect. Should be a list with [number of \"\n",
    "                \"keypoints, number of dims (2 for x,y or 3 for x,y,visible)], i.e. 'kpt_shape: [17, 3]'\"\n",
    "            )\n",
    "        with ThreadPool(NUM_THREADS) as pool:\n",
    "            results = pool.imap(\n",
    "                func=verify_image_label,\n",
    "                iterable=zip(\n",
    "                    self.im_files,\n",
    "                    self.label_files,\n",
    "                    repeat(self.prefix),\n",
    "                    repeat(self.use_keypoints),\n",
    "                    repeat(len(self.data[\"names\"])),\n",
    "                    repeat(nkpt),\n",
    "                    repeat(ndim),\n",
    "                    repeat(self.single_cls),\n",
    "                ),\n",
    "            )\n",
    "            pbar = TQDM(results, desc=desc, total=total)\n",
    "            for im_file, lb, shape, segments, keypoint, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n",
    "                nm += nm_f\n",
    "                nf += nf_f\n",
    "                ne += ne_f\n",
    "                nc += nc_f\n",
    "                if im_file:\n",
    "                    x[\"labels\"].append(\n",
    "                        {\n",
    "                            \"im_file\": im_file,\n",
    "                            \"shape\": shape,\n",
    "                            \"cls\": lb[:, 0:1],  # n, 1\n",
    "                            \"bboxes\": lb[:, 1:],  # n, 4\n",
    "                            \"segments\": segments,\n",
    "                            \"keypoints\": keypoint,\n",
    "                            \"normalized\": True,\n",
    "                            \"bbox_format\": \"xywh\",\n",
    "                        }\n",
    "                    )\n",
    "                if msg:\n",
    "                    msgs.append(msg)\n",
    "                pbar.desc = f\"{desc} {nf} images, {nm + ne} backgrounds, {nc} corrupt\"\n",
    "            pbar.close()\n",
    "\n",
    "        if msgs:\n",
    "            LOGGER.info(\"\\n\".join(msgs))\n",
    "        if nf == 0:\n",
    "            LOGGER.warning(f\"{self.prefix}No labels found in {path}. {HELP_URL}\")\n",
    "        x[\"hash\"] = get_hash(self.label_files + self.im_files)\n",
    "        x[\"results\"] = nf, nm, ne, nc, len(self.im_files)\n",
    "        x[\"msgs\"] = msgs  # warnings\n",
    "        save_dataset_cache_file(self.prefix, path, x, DATASET_CACHE_VERSION)\n",
    "        return x\n",
    "\n",
    "    def get_labels(self) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Return dictionary of labels for YOLO training.\n",
    "\n",
    "        This method loads labels from disk or cache, verifies their integrity, and prepares them for training.\n",
    "\n",
    "        Returns:\n",
    "            (List[dict]): List of label dictionaries, each containing information about an image and its annotations.\n",
    "        \"\"\"\n",
    "        self.label_files = img2label_paths(self.im_files)\n",
    "        cache_path = Path(self.label_files[0]).parent.with_suffix(\".cache\")\n",
    "        try:\n",
    "            cache, exists = load_dataset_cache_file(cache_path), True  # attempt to load a *.cache file\n",
    "            assert cache[\"version\"] == DATASET_CACHE_VERSION  # matches current version\n",
    "            assert cache[\"hash\"] == get_hash(self.label_files + self.im_files)  # identical hash\n",
    "        except (FileNotFoundError, AssertionError, AttributeError):\n",
    "            cache, exists = self.cache_labels(cache_path), False  # run cache ops\n",
    "\n",
    "        # Display cache\n",
    "        nf, nm, ne, nc, n = cache.pop(\"results\")  # found, missing, empty, corrupt, total\n",
    "        if exists and LOCAL_RANK in {-1, 0}:\n",
    "            d = f\"Scanning {cache_path}... {nf} images, {nm + ne} backgrounds, {nc} corrupt\"\n",
    "            TQDM(None, desc=self.prefix + d, total=n, initial=n)  # display results\n",
    "            if cache[\"msgs\"]:\n",
    "                LOGGER.info(\"\\n\".join(cache[\"msgs\"]))  # display warnings\n",
    "\n",
    "        # Read cache\n",
    "        [cache.pop(k) for k in (\"hash\", \"version\", \"msgs\")]  # remove items\n",
    "        labels = cache[\"labels\"]\n",
    "        if not labels:\n",
    "            raise RuntimeError(\n",
    "                f\"No valid images found in {cache_path}. Images with incorrectly formatted labels are ignored. {HELP_URL}\"\n",
    "            )\n",
    "        self.im_files = [lb[\"im_file\"] for lb in labels]  # update im_files\n",
    "\n",
    "        # Check if the dataset is all boxes or all segments\n",
    "        lengths = ((len(lb[\"cls\"]), len(lb[\"bboxes\"]), len(lb[\"segments\"])) for lb in labels)\n",
    "        len_cls, len_boxes, len_segments = (sum(x) for x in zip(*lengths))\n",
    "        if len_segments and len_boxes != len_segments:\n",
    "            LOGGER.warning(\n",
    "                f\"Box and segment counts should be equal, but got len(segments) = {len_segments}, \"\n",
    "                f\"len(boxes) = {len_boxes}. To resolve this only boxes will be used and all segments will be removed. \"\n",
    "                \"To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\"\n",
    "            )\n",
    "            for lb in labels:\n",
    "                lb[\"segments\"] = []\n",
    "        if len_cls == 0:\n",
    "            LOGGER.warning(f\"Labels are missing or empty in {cache_path}, training may not work correctly. {HELP_URL}\")\n",
    "        return labels\n",
    "\n",
    "    def build_transforms(self, hyp: Optional[Dict] = None) -> Compose:\n",
    "        \"\"\"\n",
    "        Build and append transforms to the list.\n",
    "\n",
    "        Args:\n",
    "            hyp (dict, optional): Hyperparameters for transforms.\n",
    "\n",
    "        Returns:\n",
    "            (Compose): Composed transforms.\n",
    "        \"\"\"\n",
    "        if self.augment:\n",
    "            hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\n",
    "            hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\n",
    "            hyp.cutmix = hyp.cutmix if self.augment and not self.rect else 0.0\n",
    "            transforms = v8_transforms(self, self.imgsz, hyp)\n",
    "        else:\n",
    "            transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), scaleup=False)])\n",
    "        transforms.append(\n",
    "            Format(\n",
    "                bbox_format=\"xywh\",\n",
    "                normalize=True,\n",
    "                return_mask=self.use_segments,\n",
    "                return_keypoint=self.use_keypoints,\n",
    "                return_obb=self.use_obb,\n",
    "                batch_idx=True,\n",
    "                mask_ratio=hyp.mask_ratio,\n",
    "                mask_overlap=hyp.overlap_mask,\n",
    "                bgr=hyp.bgr if self.augment else 0.0,  # only affect training.\n",
    "            )\n",
    "        )\n",
    "        return transforms\n",
    "\n",
    "    def close_mosaic(self, hyp: Dict) -> None:\n",
    "        \"\"\"\n",
    "        Disable mosaic, copy_paste, mixup and cutmix augmentations by setting their probabilities to 0.0.\n",
    "\n",
    "        Args:\n",
    "            hyp (dict): Hyperparameters for transforms.\n",
    "        \"\"\"\n",
    "        hyp.mosaic = 0.0\n",
    "        hyp.copy_paste = 0.0\n",
    "        hyp.mixup = 0.0\n",
    "        hyp.cutmix = 0.0\n",
    "        self.transforms = self.build_transforms(hyp)\n",
    "\n",
    "    def update_labels_info(self, label: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Update label format for different tasks.\n",
    "\n",
    "        Args:\n",
    "            label (dict): Label dictionary containing bboxes, segments, keypoints, etc.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Updated label dictionary with instances.\n",
    "\n",
    "        Note:\n",
    "            cls is not with bboxes now, classification and semantic segmentation need an independent cls label\n",
    "            Can also support classification and semantic segmentation by adding or removing dict keys there.\n",
    "        \"\"\"\n",
    "        bboxes = label.pop(\"bboxes\")\n",
    "        segments = label.pop(\"segments\", [])\n",
    "        keypoints = label.pop(\"keypoints\", None)\n",
    "        bbox_format = label.pop(\"bbox_format\")\n",
    "        normalized = label.pop(\"normalized\")\n",
    "\n",
    "        # NOTE: do NOT resample oriented boxes\n",
    "        segment_resamples = 100 if self.use_obb else 1000\n",
    "        if len(segments) > 0:\n",
    "            # make sure segments interpolate correctly if original length is greater than segment_resamples\n",
    "            max_len = max(len(s) for s in segments)\n",
    "            segment_resamples = (max_len + 1) if segment_resamples < max_len else segment_resamples\n",
    "            # list[np.array(segment_resamples, 2)] * num_samples\n",
    "            segments = np.stack(resample_segments(segments, n=segment_resamples), axis=0)\n",
    "        else:\n",
    "            segments = np.zeros((0, segment_resamples, 2), dtype=np.float32)\n",
    "        label[\"instances\"] = Instances(bboxes, segments, keypoints, bbox_format=bbox_format, normalized=normalized)\n",
    "        return label\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch: List[Dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Collate data samples into batches.\n",
    "\n",
    "        Args:\n",
    "            batch (List[dict]): List of dictionaries containing sample data.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Collated batch with stacked tensors.\n",
    "        \"\"\"\n",
    "        new_batch = {}\n",
    "        batch = [dict(sorted(b.items())) for b in batch]  # make sure the keys are in the same order\n",
    "        keys = batch[0].keys()\n",
    "        values = list(zip(*[list(b.values()) for b in batch]))\n",
    "        for i, k in enumerate(keys):\n",
    "            value = values[i]\n",
    "            if k in {\"img\", \"text_feats\"}:\n",
    "                value = torch.stack(value, 0)\n",
    "            elif k == \"visuals\":\n",
    "                value = torch.nn.utils.rnn.pad_sequence(value, batch_first=True)\n",
    "            if k in {\"masks\", \"keypoints\", \"bboxes\", \"cls\", \"segments\", \"obb\"}:\n",
    "                value = torch.cat(value, 0)\n",
    "            new_batch[k] = value\n",
    "        new_batch[\"batch_idx\"] = list(new_batch[\"batch_idx\"])\n",
    "        for i in range(len(new_batch[\"batch_idx\"])):\n",
    "            new_batch[\"batch_idx\"][i] += i  # add target image index for build_targets()\n",
    "        new_batch[\"batch_idx\"] = torch.cat(new_batch[\"batch_idx\"], 0)\n",
    "        return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae322fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.cfg import IterableSimpleNamespace\n",
    "from typing import Any, Dict\n",
    "from ultralytics.utils import colorstr\n",
    "\n",
    "def build_yolo_dataset(\n",
    "    cfg: IterableSimpleNamespace,\n",
    "    img_path: str,\n",
    "    batch: int,\n",
    "    data: Dict[str, Any],\n",
    "    mode: str = \"train\",\n",
    "    rect: bool = False,\n",
    "    stride: int = 32,\n",
    "    multi_modal: bool = False,\n",
    "):\n",
    "    \n",
    "    print(f\"\\n\\nThis is image path {img_path}\")\n",
    "    \"\"\"Build and return a YOLO dataset based on configuration parameters.\"\"\"\n",
    "    dataset = CustomYOLODataset # YOLOMultiModalDataset if multi_modal else CustomYOLODataset\n",
    "    return dataset(\n",
    "        img_path=img_path,\n",
    "        imgsz=cfg.imgsz,\n",
    "        batch_size=batch,\n",
    "        augment=mode == \"train\",  # augmentation\n",
    "        hyp=cfg,  # TODO: probably add a get_hyps_from_cfg function\n",
    "        rect=cfg.rect or rect,  # rectangular batches\n",
    "        cache=cfg.cache or None,\n",
    "        single_cls=cfg.single_cls or False,\n",
    "        stride=int(stride),\n",
    "        pad=0.0 if mode == \"train\" else 0.5,\n",
    "        prefix=colorstr(f\"{mode}: \"),\n",
    "        task=cfg.task,\n",
    "        classes=cfg.classes,\n",
    "        data=data,\n",
    "        fraction=cfg.fraction if mode == \"train\" else 1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eca0996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.nn.modules import Detect, Segment, YOLOESegment, Pose, OBB\n",
    "from ultralytics.utils.torch_utils import initialize_weights, de_parallel\n",
    "from ultralytics.nn.tasks import yaml_model_load, parse_model\n",
    "from ultralytics.nn.tasks import DetectionModel \n",
    "from ultralytics.utils import RANK, LOGGER\n",
    "from copy import deepcopy\n",
    "\n",
    "class CustomDetectionModel(DetectionModel): \n",
    "    def __init__(self, cfg=\"yolo11s.yaml\", ch=4, nc=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize the YOLO detection model with the given config and parameters.\n",
    "\n",
    "        Args:\n",
    "            cfg (str | dict): Model configuration file path or dictionary.\n",
    "            ch (int): Number of input channels.\n",
    "            nc (int, optional): Number of classes.\n",
    "            verbose (bool): Whether to display model information.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # cfg dict\n",
    "        if self.yaml[\"backbone\"][0][2] == \"Silence\":\n",
    "            LOGGER.warning(\n",
    "                \"YOLOv9 `Silence` module is deprecated in favor of torch.nn.Identity. \"\n",
    "                \"Please delete local *.pt file and re-download the latest model checkpoint.\"\n",
    "            )\n",
    "            self.yaml[\"backbone\"][0][2] = \"nn.Identity\"\n",
    "\n",
    "        # Define model\n",
    "        self.yaml[\"channels\"] = ch  # save channels\n",
    "        if nc and nc != self.yaml[\"nc\"]:\n",
    "            LOGGER.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n",
    "            self.yaml[\"nc\"] = nc  # override YAML value\n",
    "        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # model, savelist\n",
    "        self.names = {i: f\"{i}\" for i in range(self.yaml[\"nc\"])}  # default names dict\n",
    "        self.inplace = self.yaml.get(\"inplace\", True)\n",
    "        self.end2end = getattr(self.model[-1], \"end2end\", False)\n",
    "\n",
    "        # Build strides\n",
    "        m = self.model[-1]  # Detect()\n",
    "        if isinstance(m, Detect):  # includes all Detect subclasses like Segment, Pose, OBB, YOLOEDetect, YOLOESegment\n",
    "            s = 256  # 2x min stride\n",
    "            m.inplace = self.inplace\n",
    "\n",
    "            def _forward(x):\n",
    "                \"\"\"Perform a forward pass through the model, handling different Detect subclass types accordingly.\"\"\"\n",
    "                if self.end2end:\n",
    "                    return self.forward(x)[\"one2many\"]\n",
    "                return self.forward(x)[0] if isinstance(m, (Segment, YOLOESegment, Pose, OBB)) else self.forward(x)\n",
    "\n",
    "            self.model.eval()  # Avoid changing batch statistics until training begins\n",
    "            m.training = True  # Setting it to True to properly return strides\n",
    "            m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # forward\n",
    "            self.stride = m.stride\n",
    "            self.model.train()  # Set model back to training(default) mode\n",
    "            m.bias_init()  # only run once\n",
    "        else:\n",
    "            self.stride = torch.Tensor([32])  # default stride for i.e. RTDETR\n",
    "\n",
    "        # Init weights, biases\n",
    "        initialize_weights(self)\n",
    "        if verbose:\n",
    "            self.info()\n",
    "            LOGGER.info(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73379303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics.engine.trainer import BaseTrainer\n",
    "from typing import Optional\n",
    "\n",
    "class CustomDetectionTrainer(DetectionTrainer):\n",
    "    \"\"\"ORIGINALLY INHERITS FROM THE DetectionTrainer, WHICH ORIGINALLY INHERITS FROM THE BaseTrainer\"\"\"\n",
    "    def build_dataset(self, img_path: str, mode: str = \"train\", batch: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Build YOLO Dataset for training or validation.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): Path to the folder containing images.\n",
    "            mode (str): 'train' mode or 'val' mode, users are able to customize different augmentations for each mode.\n",
    "            batch (int, optional): Size of batches, this is for 'rect' mode.\n",
    "\n",
    "        Returns:\n",
    "            (Dataset): YOLO dataset object configured for the specified mode.\n",
    "        \"\"\"\n",
    "        gs = max(int(de_parallel(self.model).stride.max() if self.model else 0), 32)\n",
    "        return build_yolo_dataset(self.args, img_path, batch, self.data, mode=mode, rect=mode == \"val\", stride=gs)\n",
    "\n",
    "    def get_model(self, cfg: Optional[str] = None, weights: Optional[str] = None, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Return a YOLO detection model.\n",
    "\n",
    "        Args:\n",
    "            cfg (str, optional): Path to model configuration file.\n",
    "            weights (str, optional): Path to model weights.\n",
    "            verbose (bool): Whether to display model information.\n",
    "\n",
    "        Returns:\n",
    "            (DetectionModel): YOLO detection model.\n",
    "        \"\"\"\n",
    "        model = CustomDetectionModel(cfg, nc=self.data[\"nc\"], ch=4, verbose=verbose and RANK == -1)\n",
    "        if weights:\n",
    "            model.load(weights)\n",
    "        return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e589cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.153 ðŸš€ Python-3.12.11 torch-2.7.1+cu128 CUDA:0 (NVIDIA RTX A4000, 14961MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=512, bgr=0.0, box=6.5, cache=False, cfg=None, classes=None, close_mosaic=0, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=datasets/data.yaml, degrees=0.0, deterministic=True, device=None, dfl=2.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=0, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=192, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=1e-05, lrf=1e-05, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolo11_4ch_data, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=False, profile=False, project=train_yolo11_4ch_2025_08_04_16_30_11, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=True, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding class names with single class.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    464912  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLO11n summary: 181 layers, 2,624,080 parameters, 2,624,064 gradients, 6.6 GFLOPs\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1216  ultralytics.nn.modules.conv.Conv             [4, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n",
      "  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n",
      " 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n",
      " 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 23        [16, 19, 22]  1    819795  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "YOLO11s summary: 181 layers, 9,428,467 parameters, 9,428,451 gradients, 21.6 GFLOPs\n",
      "\n",
      "Transferred 493/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\n",
      "\n",
      "This is image path /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/images/train\n",
      "\n",
      "\n",
      "This is args in CustomYOLODataset: ()\n",
      "This is kwargs in CustomYOLODataset: {'img_path': '/home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/images/train', 'imgsz': 192, 'batch_size': 512, 'augment': True, 'hyp': IterableSimpleNamespace(task='detect', mode='train', model='yolo11s.pt', data='datasets/data.yaml', epochs=10, time=None, patience=100, batch=512, imgsz=192, save=True, save_period=-1, cache=False, device=None, workers=8, project='train_yolo11_4ch_2025_08_04_16_30_11', name='yolo11_4ch_data', exist_ok=False, pretrained=False, optimizer='auto', verbose=True, seed=42, deterministic=True, single_cls=True, rect=False, cos_lr=True, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, freeze=0, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split='val', save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format='torchscript', keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=1e-05, lrf=1e-05, momentum=0.937, weight_decay=0.0005, warmup_epochs=3, warmup_momentum=0.8, warmup_bias_lr=0.1, box=6.5, cls=0.5, dfl=2.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode='flip', auto_augment='randaugment', erasing=0.4, cfg=None, tracker='botsort.yaml', save_dir='train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data'), 'rect': False, 'cache': None, 'single_cls': True, 'stride': 32, 'pad': 0.0, 'prefix': '\\x1b[34m\\x1b[1mtrain: \\x1b[0m', 'classes': None, 'fraction': 1.0}\n",
      "\n",
      "\n",
      "This is img_path /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/images/train\n",
      "\n",
      "\n",
      "This is channels 4\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 1019.6Â±595.3 MB/s, size: 31.8 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/labels/train.cache... 6510 images, 3242 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6510/6510 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is image path /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/images/val\n",
      "\n",
      "\n",
      "This is args in CustomYOLODataset: ()\n",
      "This is kwargs in CustomYOLODataset: {'img_path': '/home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/images/val', 'imgsz': 192, 'batch_size': 1024, 'augment': False, 'hyp': IterableSimpleNamespace(task='detect', mode='train', model='yolo11s.pt', data='datasets/data.yaml', epochs=10, time=None, patience=100, batch=512, imgsz=192, save=True, save_period=-1, cache=False, device=None, workers=8, project='train_yolo11_4ch_2025_08_04_16_30_11', name='yolo11_4ch_data', exist_ok=False, pretrained=False, optimizer='auto', verbose=True, seed=42, deterministic=True, single_cls=True, rect=False, cos_lr=True, close_mosaic=0, resume=False, amp=True, fraction=1.0, profile=False, freeze=0, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split='val', save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format='torchscript', keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=1e-05, lrf=1e-05, momentum=0.937, weight_decay=0.0005, warmup_epochs=3, warmup_momentum=0.8, warmup_bias_lr=0.1, box=6.5, cls=0.5, dfl=2.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, cutmix=0.0, copy_paste=0.0, copy_paste_mode='flip', auto_augment='randaugment', erasing=0.4, cfg=None, tracker='botsort.yaml', save_dir='train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data'), 'rect': True, 'cache': None, 'single_cls': True, 'stride': 32, 'pad': 0.5, 'prefix': '\\x1b[34m\\x1b[1mval: \\x1b[0m', 'classes': None, 'fraction': 1.0}\n",
      "\n",
      "\n",
      "This is img_path /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/images/val\n",
      "\n",
      "\n",
      "This is channels 4\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 862.8Â±580.3 MB/s, size: 27.5 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jun/Desktop/inspirit/yolo_unet/train_yolo/datasets/stacked_detection/labels/val.cache... 1395 images, 848 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1395/1395 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=1e-05' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.004), 87 bias(decay=0.0)\n",
      "Image sizes 192 train, 192 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mtrain_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      11.3G      2.018      4.446       3.38        326        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.22it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.119       0.17     0.0666     0.0241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      10.7G      1.155      1.565      2.081        298        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.781      0.425      0.492      0.271\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      10.7G      0.969     0.8764       1.82        311        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547     0.0771      0.386      0.044     0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      10.7G     0.9017     0.7466      1.776        300        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.111      0.236     0.0446     0.0232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      10.7G     0.8469     0.6915      1.733        312        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.639      0.267      0.246      0.152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      10.7G     0.8269      0.646      1.708        301        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547       0.75      0.373       0.46      0.242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      10.7G      0.791     0.6142      1.682        325        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.826      0.495      0.617       0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      10.7G      0.744     0.5615      1.653        382        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547      0.879      0.727      0.797      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      10.7G      0.707     0.5339      1.638        308        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:06<00:00,  1.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547       0.91      0.759      0.854      0.624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      10.7G     0.6857     0.4991      1.615        334        192: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:07<00:00,  1.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547        0.9      0.773      0.855      0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.033 hours.\n",
      "Optimizer stripped from train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data/weights/last.pt, 19.1MB\n",
      "Optimizer stripped from train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data/weights/best.pt, 19.1MB\n",
      "\n",
      "Validating train_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data/weights/best.pt...\n",
      "Ultralytics 8.3.153 ðŸš€ Python-3.12.11 torch-2.7.1+cu128 CUDA:0 (NVIDIA RTX A4000, 14961MiB)\n",
      "YOLO11s summary (fused): 100 layers, 9,413,475 parameters, 0 gradients, 21.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1395        547        0.9      0.773      0.855      0.629\n",
      "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mtrain_yolo11_4ch_2025_08_04_16_30_11/yolo11_4ch_data\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from parameters import *\n",
    "\n",
    "def GetCurrentTime(): \n",
    "    current_time = time.localtime()\n",
    "    return time.strftime(\"%Y_%m_%d_%H_%M_%S\", current_time)\n",
    "\n",
    "args = dict(model=\"yolo11s.pt\", \n",
    "            data=\"datasets/data.yaml\", \n",
    "            epochs=EPOCH, \n",
    "            pretrained=PRETRAINED, \n",
    "            imgsz=IMAGE_SIZE, \n",
    "            single_cls=SINGLE_CLS, \n",
    "            close_mosaic=CLOSE_MOSAIC, \n",
    "            fraction=FRACTION,\n",
    "            freeze=FREEZE,  \n",
    "            lr0=INITIAL_LR, \n",
    "            lrf=FINAL_LR, \n",
    "            warmup_epochs=WARMUP_EPOCH, \n",
    "            cls=CLS, \n",
    "            box=BOX, \n",
    "            dfl=DFL, \n",
    "            seed=SEED, \n",
    "            batch=BATCH,\n",
    "            amp=MIX_PRECISION, \n",
    "            multi_scale=MULTI_SCALE, \n",
    "            cos_lr=COS_LR,\n",
    "            plots=PLOT,\n",
    "            profile=PROFILE,\n",
    "            project=f\"{MODE}_{MODEL}_{GetCurrentTime()}\",\n",
    "            name=f\"{MODEL}_{DATASET}\", )\n",
    "trainer = CustomDetectionTrainer(overrides=args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a7e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomDetectionModel(\n",
      "  (model): Sequential(\n",
      "    (0): Conv(\n",
      "      (conv): Conv2d(4, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (1): Conv(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (2): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Conv(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (4): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Conv(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (6): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): C3k(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (7): Conv(\n",
      "      (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (8): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): C3k(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): SPPF(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (10): C2PSA(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): Sequential(\n",
      "        (0): PSABlock(\n",
      "          (attn): Attention(\n",
      "            (qkv): Conv(\n",
      "              (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "            (proj): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "            (pe): Conv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "          (ffn): Sequential(\n",
      "            (0): Conv(\n",
      "              (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (11): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (12): Concat()\n",
      "    (13): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (14): Upsample(scale_factor=2.0, mode='nearest')\n",
      "    (15): Concat()\n",
      "    (16): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(32, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (17): Conv(\n",
      "      (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (18): Concat()\n",
      "    (19): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): Bottleneck(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (20): Conv(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "      (act): SiLU(inplace=True)\n",
      "    )\n",
      "    (21): Concat()\n",
      "    (22): C3k2(\n",
      "      (cv1): Conv(\n",
      "        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (cv2): Conv(\n",
      "        (conv): Conv2d(768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "        (act): SiLU(inplace=True)\n",
      "      )\n",
      "      (m): ModuleList(\n",
      "        (0): C3k(\n",
      "          (cv1): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv2): Conv(\n",
      "            (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (cv3): Conv(\n",
      "            (conv): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (m): Sequential(\n",
      "            (0): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "            (1): Bottleneck(\n",
      "              (cv1): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "              (cv2): Conv(\n",
      "                (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "                (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "                (act): SiLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (23): Detect(\n",
      "      (cv2): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Conv(\n",
      "            (conv): Conv2d(512, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv(\n",
      "            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "            (act): SiLU(inplace=True)\n",
      "          )\n",
      "          (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (cv3): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
      "              (bn): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
      "              (bn): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Sequential(\n",
      "            (0): DWConv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "            (1): Conv(\n",
      "              (conv): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (bn): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
      "              (act): SiLU(inplace=True)\n",
      "            )\n",
      "          )\n",
      "          (2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (dfl): DFL(\n",
      "        (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f1e3f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
